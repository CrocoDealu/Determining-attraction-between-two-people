{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:28:22.892116Z",
     "start_time": "2025-11-17T15:28:22.887541Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:21:22.524891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cccff6487fd23bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:18:17.392185Z",
     "start_time": "2025-11-17T15:18:17.244342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sessions: ['NateAlexis', 'EliGianna', 'DaemahniGianna', 'SarahTexas', 'StephenMiette', 'ZahariahErin', 'ChaseGianna', 'StephenKeala', 'MarshallBritney']\n",
      "\n",
      "=== Processing Session: NateAlexis ===\n",
      "People in session: ['Alexis', 'Nate']\n",
      "  Loading data for Alexis in session NateAlexis\n",
      "    Action Units: (193, 18)\n",
      "    Hand Gestures: (104, 15)\n",
      "    Audio Features: (8318, 12)\n",
      "    Alexis final shape: (177, 27)\n",
      "    Alexis features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Nate in session NateAlexis\n",
      "    Action Units: (193, 18)\n",
      "    Hand Gestures: (28, 15)\n",
      "    Audio Features: (8318, 12)\n",
      "    Nate final shape: (119, 27)\n",
      "    Nate features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "=== Processing Session: EliGianna ===\n",
      "People in session: ['Eli', 'Gianna']\n",
      "  Loading data for Eli in session EliGianna\n",
      "    Action Units: (72, 18)\n",
      "    Hand Gestures: (35, 15)\n",
      "    Audio Features: (3223, 12)\n",
      "    Eli final shape: (65, 27)\n",
      "    Eli features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Gianna in session EliGianna\n",
      "    Action Units: (74, 18)\n",
      "    Hand Gestures: (12, 15)\n",
      "    Audio Features: (3223, 12)\n",
      "    Gianna final shape: (65, 27)\n",
      "    Gianna features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "=== Processing Session: DaemahniGianna ===\n",
      "People in session: ['Daemahni', 'Gianna']\n",
      "  Loading data for Daemahni in session DaemahniGianna\n",
      "    Action Units: (85, 18)\n",
      "    Hand Gestures: (253, 15)\n",
      "    Audio Features: (3656, 12)\n",
      "    Sentiment (aggregated): (83, 5)\n",
      "    Daemahni final shape: (83, 31)\n",
      "    Daemahni features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "  Loading data for Gianna in session DaemahniGianna\n",
      "    Action Units: (85, 18)\n",
      "    Hand Gestures: (253, 15)\n",
      "    Audio Features: (3656, 12)\n",
      "    Sentiment (aggregated): (83, 5)\n",
      "    Gianna final shape: (83, 31)\n",
      "    Gianna features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "\n",
      "=== Processing Session: SarahTexas ===\n",
      "People in session: ['Texas', 'Sarah']\n",
      "  Loading data for Texas in session SarahTexas\n",
      "    Action Units: (192, 18)\n",
      "    Hand Gestures: (86, 15)\n",
      "    Audio Features: (8261, 12)\n",
      "    Texas final shape: (168, 27)\n",
      "    Texas features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Sarah in session SarahTexas\n",
      "    Action Units: (192, 18)\n",
      "    Hand Gestures: (197, 15)\n",
      "    Audio Features: (8261, 12)\n",
      "    Sarah final shape: (183, 27)\n",
      "    Sarah features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "=== Processing Session: StephenMiette ===\n",
      "People in session: ['Stephen', 'Miette']\n",
      "  Loading data for Stephen in session StephenMiette\n",
      "    Action Units: (140, 18)\n",
      "    Hand Gestures: (22, 15)\n",
      "    Audio Features: (6400, 12)\n",
      "    Stephen final shape: (108, 27)\n",
      "    Stephen features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Miette in session StephenMiette\n",
      "    Action Units: (146, 18)\n",
      "    Hand Gestures: (42, 15)\n",
      "    Audio Features: (6400, 12)\n",
      "    Miette final shape: (86, 27)\n",
      "    Miette features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "=== Processing Session: ZahariahErin ===\n",
      "People in session: ['Zahariah', 'Erin']\n",
      "  Loading data for Zahariah in session ZahariahErin\n",
      "    Action Units: (151, 18)\n",
      "    Hand Gestures: (68, 15)\n",
      "    Audio Features: (6770, 12)\n",
      "    Zahariah final shape: (119, 27)\n",
      "    Zahariah features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Erin in session ZahariahErin\n",
      "    Action Units: (151, 18)\n",
      "    Hand Gestures: (202, 15)\n",
      "    Audio Features: (6770, 12)\n",
      "    Erin final shape: (144, 27)\n",
      "    Erin features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "=== Processing Session: ChaseGianna ===\n",
      "People in session: ['Chase', 'Gianna']\n",
      "  Loading data for Chase in session ChaseGianna\n",
      "    Action Units: (30, 18)\n",
      "    Hand Gestures: (16, 15)\n",
      "    Audio Features: (1341, 12)\n",
      "    Chase final shape: (13, 27)\n",
      "    Chase features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Gianna in session ChaseGianna\n",
      "    Action Units: (30, 18)\n",
      "    Hand Gestures: (15, 15)\n",
      "    Audio Features: (1341, 12)\n",
      "    Gianna final shape: (24, 27)\n",
      "    Gianna features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "=== Processing Session: StephenKeala ===\n",
      "People in session: ['Keala', 'Stephen']\n",
      "  Loading data for Keala in session StephenKeala\n",
      "    Action Units: (80, 18)\n",
      "    Hand Gestures: (239, 15)\n",
      "    Audio Features: (4527, 12)\n",
      "    Sentiment (aggregated): (87, 5)\n",
      "    Keala final shape: (80, 31)\n",
      "    Keala features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "  Loading data for Stephen in session StephenKeala\n",
      "    Action Units: (101, 18)\n",
      "    Hand Gestures: (301, 15)\n",
      "    Audio Features: (4527, 12)\n",
      "    Sentiment (aggregated): (87, 5)\n",
      "    Stephen final shape: (88, 31)\n",
      "    Stephen features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "\n",
      "=== Processing Session: MarshallBritney ===\n",
      "People in session: ['Britney', 'Marshall']\n",
      "  Loading data for Britney in session MarshallBritney\n",
      "    Action Units: (131, 18)\n",
      "    Hand Gestures: (63, 15)\n",
      "    Audio Features: (6055, 12)\n",
      "    Britney final shape: (116, 27)\n",
      "    Britney features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "  Loading data for Marshall in session MarshallBritney\n",
      "    Action Units: (131, 18)\n",
      "    Hand Gestures: (17, 15)\n",
      "    Audio Features: (6055, 12)\n",
      "    Marshall final shape: (65, 27)\n",
      "    Marshall features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate']\n",
      "\n",
      "============================================================\n",
      "FINAL DATA SUMMARY\n",
      "============================================================\n",
      "NateAlexis_Alexis:\n",
      "  Shape: (177, 27)\n",
      "  Duration: 176 seconds\n",
      "  Features: 24\n",
      "\n",
      "NateAlexis_Nate:\n",
      "  Shape: (119, 27)\n",
      "  Duration: 118 seconds\n",
      "  Features: 24\n",
      "\n",
      "EliGianna_Eli:\n",
      "  Shape: (65, 27)\n",
      "  Duration: 64 seconds\n",
      "  Features: 24\n",
      "\n",
      "EliGianna_Gianna:\n",
      "  Shape: (65, 27)\n",
      "  Duration: 64 seconds\n",
      "  Features: 24\n",
      "\n",
      "DaemahniGianna_Daemahni:\n",
      "  Shape: (83, 31)\n",
      "  Duration: 82 seconds\n",
      "  Features: 28\n",
      "\n",
      "DaemahniGianna_Gianna:\n",
      "  Shape: (83, 31)\n",
      "  Duration: 82 seconds\n",
      "  Features: 28\n",
      "\n",
      "SarahTexas_Texas:\n",
      "  Shape: (168, 27)\n",
      "  Duration: 167 seconds\n",
      "  Features: 24\n",
      "\n",
      "SarahTexas_Sarah:\n",
      "  Shape: (183, 27)\n",
      "  Duration: 182 seconds\n",
      "  Features: 24\n",
      "\n",
      "StephenMiette_Stephen:\n",
      "  Shape: (108, 27)\n",
      "  Duration: 107 seconds\n",
      "  Features: 24\n",
      "\n",
      "StephenMiette_Miette:\n",
      "  Shape: (86, 27)\n",
      "  Duration: 85 seconds\n",
      "  Features: 24\n",
      "\n",
      "ZahariahErin_Zahariah:\n",
      "  Shape: (119, 27)\n",
      "  Duration: 118 seconds\n",
      "  Features: 24\n",
      "\n",
      "ZahariahErin_Erin:\n",
      "  Shape: (144, 27)\n",
      "  Duration: 143 seconds\n",
      "  Features: 24\n",
      "\n",
      "ChaseGianna_Chase:\n",
      "  Shape: (13, 27)\n",
      "  Duration: 12 seconds\n",
      "  Features: 24\n",
      "\n",
      "ChaseGianna_Gianna:\n",
      "  Shape: (24, 27)\n",
      "  Duration: 23 seconds\n",
      "  Features: 24\n",
      "\n",
      "StephenKeala_Keala:\n",
      "  Shape: (80, 31)\n",
      "  Duration: 79 seconds\n",
      "  Features: 28\n",
      "\n",
      "StephenKeala_Stephen:\n",
      "  Shape: (88, 31)\n",
      "  Duration: 87 seconds\n",
      "  Features: 28\n",
      "\n",
      "MarshallBritney_Britney:\n",
      "  Shape: (116, 27)\n",
      "  Duration: 115 seconds\n",
      "  Features: 24\n",
      "\n",
      "MarshallBritney_Marshall:\n",
      "  Shape: (65, 27)\n",
      "  Duration: 64 seconds\n",
      "  Features: 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class FinalMultimodalDataProcessor:\n",
    "    def __init__(self, base_path=\"DatasetCercetare\"):\n",
    "        self.base_path = base_path\n",
    "        self.sessions_data = {}\n",
    "\n",
    "    def get_all_sessions(self):\n",
    "        \"\"\"Get all unique session names from the file structure\"\"\"\n",
    "        # Get session names from AudioFeatures (since they're shared)\n",
    "        audio_files = glob.glob(f\"{self.base_path}/AudioFeatures/*.csv\")\n",
    "        sessions = [Path(file).stem for file in audio_files]\n",
    "        return sessions\n",
    "\n",
    "    def get_people_in_session(self, session_name):\n",
    "        \"\"\"Get the people involved in a session from ActionUnits files\"\"\"\n",
    "        au_files = glob.glob(f\"{self.base_path}/ActionUnits/*_on_{session_name}.csv\")\n",
    "        people = []\n",
    "        for file in au_files:\n",
    "            filename = Path(file).stem\n",
    "            person = filename.split('_on_')[0]\n",
    "            people.append(person)\n",
    "        return people\n",
    "\n",
    "    def load_person_data(self, person, session_name):\n",
    "        \"\"\"Load data for a specific person in a session\"\"\"\n",
    "        person_data = {}\n",
    "\n",
    "        print(f\"  Loading data for {person} in session {session_name}\")\n",
    "\n",
    "        # Load Action Units for this person\n",
    "        au_file = f\"{self.base_path}/ActionUnits/{person}_on_{session_name}.csv\"\n",
    "        if Path(au_file).exists():\n",
    "            au_df = pd.read_csv(au_file)\n",
    "            au_features = ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09',\n",
    "                          'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23',\n",
    "                          'AU25', 'AU26', 'AU28']\n",
    "            person_data['action_units'] = au_df[['timestamp'] + au_features]\n",
    "            print(f\"    Action Units: {au_df.shape}\")\n",
    "\n",
    "        # Load Hand Gestures for this person\n",
    "        hg_file = f\"{self.base_path}/HandGestures/{person}_on_{session_name}.csv\"\n",
    "        if Path(hg_file).exists():\n",
    "            hg_df = pd.read_csv(hg_file)\n",
    "            hg_features = ['left_hand_velocity', 'right_hand_velocity',\n",
    "                          'gesture_frequency_cumulative', 'face_touches_cumulative']\n",
    "            person_data['hand_gestures'] = hg_df[['timestamp'] + hg_features]\n",
    "            print(f\"    Hand Gestures: {hg_df.shape}\")\n",
    "\n",
    "        # Load shared Audio Features (same for all people in session)\n",
    "        audio_file = f\"{self.base_path}/AudioFeatures/{session_name}.csv\"\n",
    "        if Path(audio_file).exists():\n",
    "            audio_df = pd.read_csv(audio_file)\n",
    "            audio_features = ['energy_db', 'pitch_hz', 'speaking_rate']\n",
    "            audio_df = audio_df.rename(columns={'time_seconds': 'timestamp'})\n",
    "            person_data['audio'] = audio_df[['timestamp'] + audio_features]\n",
    "            print(f\"    Audio Features: {audio_df.shape}\")\n",
    "\n",
    "        # Load shared Sentiment Analysis (filter by speaker if available)\n",
    "        sent_file = f\"{self.base_path}/SentimentAnalysis/{session_name}.csv\"\n",
    "        if Path(sent_file).exists():\n",
    "            sent_df = pd.read_csv(sent_file)\n",
    "            sent_df = sent_df.rename(columns={'second': 'timestamp'})\n",
    "\n",
    "            # Filter by speaker if the person name matches\n",
    "            if 'speaker' in sent_df.columns:\n",
    "                # Try to match person name with speaker (case insensitive)\n",
    "                person_sent = sent_df[sent_df['speaker'].str.lower() == person.lower()]\n",
    "                if len(person_sent) > 0:\n",
    "                    person_data['sentiment'] = person_sent[['timestamp', 'compound', 'pos', 'neu', 'neg']]\n",
    "                    print(f\"    Sentiment (filtered for {person}): {person_sent.shape}\")\n",
    "                else:\n",
    "                    # If no match, use aggregated sentiment for all speakers\n",
    "                    sent_agg = sent_df.groupby('timestamp').agg({\n",
    "                        'compound': 'mean', 'pos': 'mean', 'neu': 'mean', 'neg': 'mean'\n",
    "                    }).reset_index()\n",
    "                    person_data['sentiment'] = sent_agg\n",
    "                    print(f\"    Sentiment (aggregated): {sent_agg.shape}\")\n",
    "            else:\n",
    "                person_data['sentiment'] = sent_df[['timestamp', 'compound', 'pos', 'neu', 'neg']]\n",
    "                print(f\"    Sentiment: {sent_df.shape}\")\n",
    "\n",
    "        return person_data\n",
    "\n",
    "    def align_person_data(self, person_data, target_fps=1.0):\n",
    "        \"\"\"Align all modalities for a person to the same temporal grid\"\"\"\n",
    "\n",
    "        # Find common time range\n",
    "        min_time = 0\n",
    "        max_time = float('inf')\n",
    "\n",
    "        for modality, data in person_data.items():\n",
    "            if len(data) > 0:\n",
    "                min_time = max(min_time, data['timestamp'].min())\n",
    "                max_time = min(max_time, data['timestamp'].max())\n",
    "\n",
    "        # Create target timeline\n",
    "        target_timeline = np.arange(int(min_time), int(max_time) + 1)\n",
    "        aligned_data = pd.DataFrame({'timestamp': target_timeline})\n",
    "\n",
    "        # Align each modality\n",
    "        for modality, data in person_data.items():\n",
    "            if modality == 'audio':\n",
    "                # Aggregate high-frequency audio to 1-second intervals\n",
    "                audio_agg = data.groupby(data['timestamp'].round()).agg({\n",
    "                    'energy_db': 'mean',\n",
    "                    'pitch_hz': 'mean',\n",
    "                    'speaking_rate': 'mean'\n",
    "                }).reset_index()\n",
    "                aligned_data = aligned_data.merge(audio_agg, on='timestamp', how='left')\n",
    "\n",
    "            else:\n",
    "                # For other modalities, use nearest second matching\n",
    "                data_rounded = data.copy()\n",
    "                data_rounded['timestamp'] = data_rounded['timestamp'].round().astype(int)\n",
    "                data_agg = data_rounded.groupby('timestamp').first().reset_index()\n",
    "                aligned_data = aligned_data.merge(data_agg, on='timestamp', how='left')\n",
    "\n",
    "        # Fill missing values\n",
    "        aligned_data = aligned_data.fillna(method='ffill').fillna(0)\n",
    "\n",
    "        return aligned_data\n",
    "\n",
    "    def process_all_data(self):\n",
    "        \"\"\"Process all sessions and people\"\"\"\n",
    "        sessions = self.get_all_sessions()\n",
    "        print(f\"Found sessions: {sessions}\")\n",
    "\n",
    "        all_processed_data = {}\n",
    "\n",
    "        for session in sessions:\n",
    "            print(f\"\\n=== Processing Session: {session} ===\")\n",
    "            people = self.get_people_in_session(session)\n",
    "            print(f\"People in session: {people}\")\n",
    "\n",
    "            session_data = {}\n",
    "\n",
    "            for person in people:\n",
    "                # Load person's data\n",
    "                person_data = self.load_person_data(person, session)\n",
    "\n",
    "                # Align temporal data\n",
    "                aligned_data = self.align_person_data(person_data)\n",
    "\n",
    "                # Add person and session info\n",
    "                aligned_data['person'] = person\n",
    "                aligned_data['session'] = session\n",
    "\n",
    "                print(f\"    {person} final shape: {aligned_data.shape}\")\n",
    "                print(f\"    {person} features: {[col for col in aligned_data.columns if col not in ['timestamp', 'person', 'session']]}\")\n",
    "\n",
    "                session_data[person] = aligned_data\n",
    "                all_processed_data[f\"{session}_{person}\"] = aligned_data\n",
    "\n",
    "            self.sessions_data[session] = session_data\n",
    "\n",
    "        return all_processed_data\n",
    "\n",
    "# Process all data\n",
    "processor = FinalMultimodalDataProcessor()\n",
    "all_data = processor.process_all_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, data in all_data.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Duration: {data['timestamp'].max() - data['timestamp'].min():.0f} seconds\")\n",
    "    print(f\"  Features: {len([col for col in data.columns if col not in ['timestamp', 'person', 'session']])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e765c39d32a47d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:22:06.111049Z",
     "start_time": "2025-11-17T15:22:06.105342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Labels:\n",
      "              session_person  is_attracted\n",
      "0          NateAlexis_Alexis             1\n",
      "1    MarshallBritney_Britney             1\n",
      "2          ChaseGianna_Chase             1\n",
      "3    DaemahniGianna_Daemahni             1\n",
      "4              EliGianna_Eli             1\n",
      "5          ZahariahErin_Erin             0\n",
      "6         ChaseGianna_Gianna             0\n",
      "7      DaemahniGianna_Gianna             1\n",
      "8           EliGianna_Gianna             0\n",
      "9         StephenKeala_Keala             0\n",
      "10  MarshallBritney_Marshall             1\n",
      "11      StephenMiette_Miette             1\n",
      "12           NateAlexis_Nate             0\n",
      "13          SarahTexas_Sarah             1\n",
      "14      StephenKeala_Stephen             0\n",
      "15     StephenMiette_Stephen             1\n",
      "16          SarahTexas_Texas             1\n",
      "17     ZahariahErin_Zahariah             1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth = {\n",
    "    'session_person': [\n",
    "        'NateAlexis_Alexis',\n",
    "        'MarshallBritney_Britney',\n",
    "        'ChaseGianna_Chase',\n",
    "        'DaemahniGianna_Daemahni',\n",
    "        'EliGianna_Eli',\n",
    "        'ZahariahErin_Erin',\n",
    "        'ChaseGianna_Gianna',\n",
    "        'DaemahniGianna_Gianna',\n",
    "        'EliGianna_Gianna',\n",
    "        'StephenKeala_Keala',\n",
    "        'MarshallBritney_Marshall',\n",
    "        'StephenMiette_Miette',\n",
    "        'NateAlexis_Nate',\n",
    "        'SarahTexas_Sarah',\n",
    "        'StephenKeala_Stephen',\n",
    "        'StephenMiette_Stephen',\n",
    "        'SarahTexas_Texas',\n",
    "        'ZahariahErin_Zahariah'\n",
    "    ],\n",
    "    'is_attracted': [\n",
    "        1,  # Alexis_on_NateAlexis\n",
    "        1,  # Britney_on_MarshallBritney\n",
    "        1,  # Chase_on_ChaseGianna\n",
    "        1,  # Daemahni_on_DaemahniGianna\n",
    "        1,  # Eli_on_EliGianna\n",
    "        0,  # Erin_on_ZahariahErin\n",
    "        0,  # Gianna_on_ChaseGianna\n",
    "        1,  # Gianna_on_DaemahniGianna\n",
    "        0,  # Gianna_on_EliGianna\n",
    "        0,  # Keala_on_StephenKeala\n",
    "        1,  # Marshall_on_MarshallBritney\n",
    "        1,  # Miette_on_StephenMiette\n",
    "        0,  # Nate_on_NateAlexis\n",
    "        1,  # Sarah_on_SarahTexas\n",
    "        0,  # Stephen_on_StephenKeala\n",
    "        1,  # Stephen_on_StephenMiette\n",
    "        1,  # Texas_on_SarahTexas\n",
    "        1   # Zahariah_on_ZahariahErin\n",
    "    ]\n",
    "}\n",
    "\n",
    "ground_truth_df = pd.DataFrame(ground_truth)\n",
    "print(\"Ground Truth Labels:\")\n",
    "print(ground_truth_df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd5872cf8d9fc21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:24:28.799202Z",
     "start_time": "2025-11-17T15:24:28.750806Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultimodalDataPreprocessor:\n",
    "    \"\"\"Handles data preprocessing and normalization for multimodal attraction data\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length=15):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # 1. HARDCODED CANONICAL FEATURE LIST\n",
    "        self.canonical_feature_names = [\n",
    "            'AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', \n",
    "            'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', \n",
    "            'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', \n",
    "            'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg'\n",
    "        ]\n",
    "\n",
    "    def create_sequences(self, all_data, ground_truth_df):\n",
    "        \"\"\"Create sequences for RNN training\"\"\"\n",
    "        X_sequences = []\n",
    "        y_labels = []\n",
    "        sequence_info = []\n",
    "\n",
    "        print(\"Creating sequences...\")\n",
    "        \n",
    "        # Use the hardcoded canonical list\n",
    "        self.feature_names = self.canonical_feature_names\n",
    "        \n",
    "        # Define columns to drop for clean feature data\n",
    "        non_feature_cols = ['timestamp', 'person', 'session']\n",
    "\n",
    "\n",
    "        for key, data in all_data.items():\n",
    "            # Get label for this person\n",
    "            label_row = ground_truth_df[ground_truth_df['session_person'] == key]\n",
    "            if len(label_row) == 0:\n",
    "                continue\n",
    "\n",
    "            label = label_row['is_attracted'].iloc[0]\n",
    "\n",
    "            # Remove non-feature columns\n",
    "            # Use errors='ignore' in case some datasets don't have all non-feature columns\n",
    "            feature_data = data.drop(non_feature_cols, axis=1, errors='ignore')\n",
    "\n",
    "            # 2. ENFORCE THE CANONICAL FEATURE SET\n",
    "            # Use .reindex() to ensure all 28 columns are present.\n",
    "            # If a column is missing (like 'compound' in 24-feature data), it is added and filled with 0.0.\n",
    "            feature_data = feature_data.reindex(columns=self.feature_names, fill_value=0.0)\n",
    "            \n",
    "            # --- Sanity Check (Optional but Recommended) ---\n",
    "            # Now, every feature_data MUST have exactly len(self.feature_names) columns\n",
    "            if feature_data.shape[1] != len(self.feature_names):\n",
    "                 raise RuntimeError(f\"Feature count mismatch for {key}. Expected {len(self.feature_names)}, got {feature_data.shape[1]}\")\n",
    "            # ---------------------------------------------\n",
    "\n",
    "\n",
    "            # Create overlapping sequences\n",
    "            for i in range(len(feature_data) - self.sequence_length + 1):\n",
    "                # sequence will now consistently have shape (15, 28)\n",
    "                sequence = feature_data.iloc[i:i + self.sequence_length].values\n",
    "                X_sequences.append(sequence)\n",
    "                y_labels.append(label)\n",
    "                sequence_info.append({\n",
    "                    'person': key,\n",
    "                    'start_time': i,\n",
    "                    'end_time': i + self.sequence_length - 1\n",
    "                })\n",
    "\n",
    "        # X = np.array(X_sequences) should now successfully create a 3D array of shape (N, 15, 28)\n",
    "        X = np.array(X_sequences) \n",
    "        y = np.array(y_labels)\n",
    "\n",
    "        print(f\"Created {len(X)} sequences\")\n",
    "        print(f\"Sequence shape: {X.shape}\") # Should show (N, 15, 28)\n",
    "        print(f\"Features: {len(self.feature_names)}\")\n",
    "        print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "        return X, y, sequence_info\n",
    "        \n",
    "    def fit_normalizer(self, X_train):\n",
    "        \"\"\"Fit the feature normalizer on training data\"\"\"\n",
    "        print(\"Fitting feature normalizer...\")\n",
    "\n",
    "        # Reshape for normalization (samples*time, features)\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "        # Fit scaler\n",
    "        self.feature_scaler.fit(X_train_reshaped)\n",
    "        self.is_fitted = True\n",
    "\n",
    "        print(\"Feature normalizer fitted!\")\n",
    "        return self\n",
    "\n",
    "    def normalize_features(self, X):\n",
    "        \"\"\"Normalize features using fitted scaler\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Normalizer not fitted! Call fit_normalizer() first.\")\n",
    "\n",
    "        # Reshape for normalization\n",
    "        original_shape = X.shape\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "\n",
    "        # Transform\n",
    "        X_normalized = self.feature_scaler.transform(X_reshaped)\n",
    "        X_normalized = X_normalized.reshape(original_shape)\n",
    "\n",
    "        return X_normalized\n",
    "\n",
    "    def split_by_person(self, X, y, sequence_info, validation_split=0.2):\n",
    "        \"\"\"Split data by person to avoid data leakage\"\"\"\n",
    "        print(\"Splitting data by person...\")\n",
    "\n",
    "        # Group sequences by person\n",
    "        person_sequences = {}\n",
    "        for i, info in enumerate(sequence_info):\n",
    "            person = info['person']\n",
    "            if person not in person_sequences:\n",
    "                person_sequences[person] = []\n",
    "            person_sequences[person].append(i)\n",
    "\n",
    "        # Split by person\n",
    "        train_indices = []\n",
    "        val_indices = []\n",
    "\n",
    "        for person, indices in person_sequences.items():\n",
    "            n_val = max(1, int(len(indices) * validation_split))\n",
    "            val_indices.extend(indices[-n_val:])  # Last sequences for validation\n",
    "            train_indices.extend(indices[:-n_val])  # Rest for training\n",
    "\n",
    "        X_train = X[train_indices]\n",
    "        X_val = X[val_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_val = y[val_indices]\n",
    "\n",
    "        print(f\"Train set: {len(X_train)} sequences\")\n",
    "        print(f\"Val set: {len(X_val)} sequences\")\n",
    "        print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
    "        print(f\"Val class distribution: {np.bincount(y_val)}\")\n",
    "\n",
    "        return X_train, X_val, y_train, y_val, train_indices, val_indices\n",
    "\n",
    "    def prepare_training_data(self, all_data, ground_truth_df, validation_split=0.2):\n",
    "        \"\"\"Complete data preparation pipeline\"\"\"\n",
    "        # Create sequences\n",
    "        X, y, sequence_info = self.create_sequences(all_data, ground_truth_df)\n",
    "\n",
    "        # Split by person\n",
    "        X_train, X_val, y_train, y_val, train_idx, val_idx = self.split_by_person(\n",
    "            X, y, sequence_info, validation_split\n",
    "        )\n",
    "\n",
    "        # Fit normalizer on training data\n",
    "        self.fit_normalizer(X_train)\n",
    "\n",
    "        # Normalize both sets\n",
    "        X_train_norm = self.normalize_features(X_train)\n",
    "        X_val_norm = self.normalize_features(X_val)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train_norm,\n",
    "            'X_val': X_val_norm,\n",
    "            'y_train': y_train,\n",
    "            'y_val': y_val,\n",
    "            'train_indices': train_idx,\n",
    "            'val_indices': val_idx,\n",
    "            'sequence_info': sequence_info\n",
    "        }\n",
    "\n",
    "    def preprocess_new_data(self, person_data):\n",
    "        \"\"\"Preprocess new data for prediction\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor not fitted! Train model first.\")\n",
    "\n",
    "        # Remove non-feature columns\n",
    "        feature_data = person_data.drop(['timestamp', 'person', 'session'], axis=1, errors='ignore')\n",
    "\n",
    "        # Create sequences\n",
    "        sequences = []\n",
    "        for i in range(len(feature_data) - self.sequence_length + 1):\n",
    "            sequence = feature_data.iloc[i:i + self.sequence_length].values\n",
    "            sequences.append(sequence)\n",
    "\n",
    "        if len(sequences) == 0:\n",
    "            raise ValueError(f\"Not enough data points. Need at least {self.sequence_length} time steps.\")\n",
    "\n",
    "        X = np.array(sequences)\n",
    "        X_normalized = self.normalize_features(X)\n",
    "\n",
    "        return X_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e8e73d5d69dba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:30:50.695058Z",
     "start_time": "2025-11-17T15:30:50.663580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PREPROCESSING\n",
      "============================================================\n",
      "Creating sequences...\n",
      "Created 1535 sequences\n",
      "Sequence shape: (1535, 15, 28)\n",
      "Features: 28\n",
      "Class distribution: [ 436 1099]\n",
      "Splitting data by person...\n",
      "Train set: 1235 sequences\n",
      "Val set: 300 sequences\n",
      "Train class distribution: [350 885]\n",
      "Val class distribution: [ 86 214]\n",
      "Fitting feature normalizer...\n",
      "Feature normalizer fitted!\n",
      "\n",
      "Preprocessing complete!\n",
      "Training data shape: (1235, 15, 28)\n",
      "Validation data shape: (300, 15, 28)\n",
      "Feature names: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "preprocessor = MultimodalDataPreprocessor(sequence_length=15)\n",
    "data_dict = preprocessor.prepare_training_data(all_data, ground_truth_df)\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n",
    "print(f\"Training data shape: {data_dict['X_train'].shape}\")\n",
    "print(f\"Validation data shape: {data_dict['X_val'].shape}\")\n",
    "print(f\"Feature names: {preprocessor.feature_names[:28]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f8f5fb4c4a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:29:48.018951Z",
     "start_time": "2025-11-17T15:29:48.007329Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttractionLSTMModel:\n",
    "    \"\"\"LSTM model for attraction prediction\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build LSTM model architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            # First LSTM layer\n",
    "            LSTM(64, return_sequences=True, input_shape=input_shape, dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            # Second LSTM layer\n",
    "            LSTM(32, return_sequences=True, dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            # Third LSTM layer\n",
    "            LSTM(16, dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            # Dense layers\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=16):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if self.model is None:\n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "            self.build_model(input_shape)\n",
    "\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        self.model.summary()\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=20, restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Handle class imbalance\n",
    "        class_weight = None\n",
    "        if len(np.unique(y_train)) > 1:\n",
    "            from sklearn.utils.class_weight import compute_class_weight\n",
    "            classes = np.unique(y_train)\n",
    "            weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "            class_weight = {classes[i]: weights[i] for i in range(len(classes))}\n",
    "            print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "        print(\"\\nTraining model...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        return self.model.evaluate(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154865fd3b583800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:33:09.763299Z",
     "start_time": "2025-11-17T15:32:51.628288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m23,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,897</span> (159.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,897\u001b[0m (159.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,673</span> (158.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m40,673\u001b[0m (158.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.7642857142857142, 1: 0.6977401129943502}\n",
      "\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.6235 - loss: 0.6751 - precision: 0.7869 - recall: 0.6508 - val_accuracy: 0.7567 - val_loss: 0.6249 - val_precision: 0.9029 - val_recall: 0.7383 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6866 - loss: 0.5700 - precision: 0.8716 - recall: 0.6599 - val_accuracy: 0.7667 - val_loss: 0.4608 - val_precision: 0.9557 - val_recall: 0.7056 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7814 - loss: 0.4463 - precision: 0.9463 - recall: 0.7367 - val_accuracy: 0.8167 - val_loss: 0.3534 - val_precision: 0.9162 - val_recall: 0.8178 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8105 - loss: 0.4059 - precision: 0.9429 - recall: 0.7831 - val_accuracy: 0.7933 - val_loss: 0.3667 - val_precision: 0.9130 - val_recall: 0.7850 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8243 - loss: 0.3815 - precision: 0.9418 - recall: 0.8045 - val_accuracy: 0.8100 - val_loss: 0.4436 - val_precision: 0.9758 - val_recall: 0.7523 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8640 - loss: 0.3239 - precision: 0.9614 - recall: 0.8441 - val_accuracy: 0.8833 - val_loss: 0.2620 - val_precision: 0.9050 - val_recall: 0.9346 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8802 - loss: 0.3122 - precision: 0.9670 - recall: 0.8621 - val_accuracy: 0.8967 - val_loss: 0.2436 - val_precision: 0.9336 - val_recall: 0.9206 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8899 - loss: 0.2695 - precision: 0.9664 - recall: 0.8768 - val_accuracy: 0.9167 - val_loss: 0.2182 - val_precision: 0.9479 - val_recall: 0.9346 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8931 - loss: 0.2548 - precision: 0.9642 - recall: 0.8836 - val_accuracy: 0.9200 - val_loss: 0.2021 - val_precision: 0.9657 - val_recall: 0.9206 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9117 - loss: 0.2291 - precision: 0.9743 - recall: 0.9006 - val_accuracy: 0.9300 - val_loss: 0.2212 - val_precision: 0.9754 - val_recall: 0.9252 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9344 - loss: 0.2143 - precision: 0.9786 - recall: 0.9288 - val_accuracy: 0.8900 - val_loss: 0.3827 - val_precision: 0.9502 - val_recall: 0.8925 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9255 - loss: 0.2343 - precision: 0.9783 - recall: 0.9164 - val_accuracy: 0.8933 - val_loss: 0.3360 - val_precision: 0.9505 - val_recall: 0.8972 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9441 - loss: 0.1748 - precision: 0.9766 - recall: 0.9446 - val_accuracy: 0.8867 - val_loss: 0.3682 - val_precision: 1.0000 - val_recall: 0.8411 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9247 - loss: 0.2135 - precision: 0.9748 - recall: 0.9186 - val_accuracy: 0.9067 - val_loss: 0.2868 - val_precision: 0.9650 - val_recall: 0.9019 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9385 - loss: 0.1847 - precision: 0.9776 - recall: 0.9356 - val_accuracy: 0.9133 - val_loss: 0.3098 - val_precision: 0.9653 - val_recall: 0.9112 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9474 - loss: 0.1657 - precision: 0.9824 - recall: 0.9435 - val_accuracy: 0.8600 - val_loss: 0.5371 - val_precision: 0.9257 - val_recall: 0.8738 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9385 - loss: 0.1620 - precision: 0.9798 - recall: 0.9333 - val_accuracy: 0.9033 - val_loss: 0.3945 - val_precision: 0.9557 - val_recall: 0.9065 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9401 - loss: 0.1748 - precision: 0.9754 - recall: 0.9401 - val_accuracy: 0.8900 - val_loss: 0.3795 - val_precision: 0.9641 - val_recall: 0.8785 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9441 - loss: 0.1570 - precision: 0.9857 - recall: 0.9356 - val_accuracy: 0.8400 - val_loss: 0.4527 - val_precision: 0.9560 - val_recall: 0.8131 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9587 - loss: 0.1276 - precision: 0.9826 - recall: 0.9593 - val_accuracy: 0.8667 - val_loss: 0.4566 - val_precision: 0.9728 - val_recall: 0.8364 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9425 - loss: 0.1707 - precision: 0.9822 - recall: 0.9367 - val_accuracy: 0.9067 - val_loss: 0.3581 - val_precision: 0.9471 - val_recall: 0.9206 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9652 - loss: 0.1029 - precision: 0.9895 - recall: 0.9616 - val_accuracy: 0.8933 - val_loss: 0.4285 - val_precision: 0.9375 - val_recall: 0.9112 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9587 - loss: 0.1073 - precision: 0.9929 - recall: 0.9492 - val_accuracy: 0.9233 - val_loss: 0.3168 - val_precision: 0.9569 - val_recall: 0.9346 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9660 - loss: 0.1051 - precision: 0.9907 - recall: 0.9616 - val_accuracy: 0.9033 - val_loss: 0.3403 - val_precision: 0.9469 - val_recall: 0.9159 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9571 - loss: 0.1400 - precision: 0.9837 - recall: 0.9559 - val_accuracy: 0.8900 - val_loss: 0.3929 - val_precision: 0.9548 - val_recall: 0.8879 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9644 - loss: 0.1210 - precision: 0.9918 - recall: 0.9582 - val_accuracy: 0.9067 - val_loss: 0.3796 - val_precision: 0.9429 - val_recall: 0.9252 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9619 - loss: 0.1393 - precision: 0.9850 - recall: 0.9616 - val_accuracy: 0.8767 - val_loss: 0.4397 - val_precision: 0.9492 - val_recall: 0.8738 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9652 - loss: 0.1040 - precision: 0.9861 - recall: 0.9650 - val_accuracy: 0.8900 - val_loss: 0.4029 - val_precision: 0.9372 - val_recall: 0.9065 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9555 - loss: 0.1265 - precision: 0.9929 - recall: 0.9446 - val_accuracy: 0.9000 - val_loss: 0.3898 - val_precision: 0.9510 - val_recall: 0.9065 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "attraction_model = AttractionLSTMModel()\n",
    "history = attraction_model.train(\n",
    "    data_dict['X_train'],\n",
    "    data_dict['y_train'],\n",
    "    data_dict['X_val'],\n",
    "    data_dict['y_val']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d317cd3b90db395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:35:14.480988Z",
     "start_time": "2025-11-17T15:35:13.412886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.2021\n",
      "Best validation accuracy: 0.9300\n",
      "\n",
      "Final model performance:\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9200 - loss: 0.2021 - precision: 0.9657 - recall: 0.9206 \n",
      "Validation Loss: 0.2021\n",
      "Validation Accuracy: 0.9200\n",
      "Validation Precision: 0.9657\n",
      "Validation Recall: 0.9206\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n",
      "\n",
      "Prediction distribution:\n",
      "Predicted 0 (not attracted): 96\n",
      "Predicted 1 (attracted): 204\n",
      "Actual 0 (not attracted): 86\n",
      "Actual 1 (attracted): 214\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = min(attraction_model.history.history['val_loss'])\n",
    "best_val_acc = max(attraction_model.history.history['val_accuracy'])\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nFinal model performance:\")\n",
    "val_loss, val_acc, val_precision, val_recall = attraction_model.evaluate(\n",
    "    data_dict['X_val'],\n",
    "    data_dict['y_val'],\n",
    ")\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Test predictions on validation set\n",
    "val_predictions = attraction_model.predict(data_dict['X_val'])\n",
    "val_pred_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(f\"Predicted 0 (not attracted): {np.sum(val_pred_binary == 0)}\")\n",
    "print(f\"Predicted 1 (attracted): {np.sum(val_pred_binary == 1)}\")\n",
    "print(f\"Actual 0 (not attracted): {np.sum(data_dict['y_val'] == 0)}\")\n",
    "print(f\"Actual 1 (attracted): {np.sum(data_dict['y_val'] == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a98251e7b07a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final model performance:\n",
    "10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.9200 - loss: 0.2021 - precision: 0.9657 - recall: 0.9206 \n",
    "Validation Loss: 0.2021\n",
    "Validation Accuracy: 0.9200\n",
    "Validation Precision: 0.9657\n",
    "Validation Recall: 0.9206\n",
    "10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 53ms/step\n",
    "\n",
    "Prediction distribution:\n",
    "Predicted 0 (not attracted): 96\n",
    "Predicted 1 (attracted): 204\n",
    "Actual 0 (not attracted): 86\n",
    "Actual 1 (attracted): 214"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
