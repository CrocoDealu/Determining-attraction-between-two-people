{
 "cells": [
  {
   "cell_type": "code",
   "id": "db6543d2dceed081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:08:37.262075Z",
     "start_time": "2025-11-15T15:08:37.255573Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_valid_time_intervals(video_path, threshold=10, min_duration=0.1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    timestamp = 0\n",
    "    valid_intervals = []\n",
    "    start_time = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        mean_intensity = np.mean(gray)\n",
    "\n",
    "        if mean_intensity > threshold:  # Not black\n",
    "            if start_time is None:\n",
    "                start_time = timestamp\n",
    "        else:  # Black frame\n",
    "            if start_time is not None:\n",
    "                duration = timestamp - start_time\n",
    "                if duration >= min_duration:  # Ignore very short flashes\n",
    "                    valid_intervals.append((start_time, timestamp))\n",
    "                start_time = None\n",
    "        timestamp += 1 / fps\n",
    "\n",
    "    # Handle case where video ends on valid segment\n",
    "    if start_time is not None:\n",
    "        valid_intervals.append((start_time, timestamp))\n",
    "\n",
    "    cap.release()\n",
    "    return valid_intervals  # e.g., [(2.5, 8.3), (12.1, 15.7), ...]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c9984c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:08:41.706888Z",
     "start_time": "2025-11-15T15:08:39.719068Z"
    }
   },
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load full audio (same timeline as video)\n",
    "audio = AudioSegment.from_wav(\"DatasetCercetare/Audio/Daemahni_Gianna.wav\")\n",
    "\n",
    "# Extract audio during non-black periods\n",
    "valid_intervals = get_valid_time_intervals(\"DatasetCercetare/Videos/Daemahni_on_DaemahniGianna.mov\")\n",
    "target_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "for start_sec, end_sec in valid_intervals:\n",
    "    start_ms = int(start_sec * 1000)\n",
    "    end_ms = int(end_sec * 1000)\n",
    "    target_audio += audio[start_ms:end_ms]\n",
    "\n",
    "target_audio.export(\"target_person_audio.wav\", format=\"wav\")"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m audio = AudioSegment.from_wav(\u001B[33m\"\u001B[39m\u001B[33mDatasetCercetare/Audio/Daemahni_Gianna.wav\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Extract audio during non-black periods\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m valid_intervals = \u001B[43mget_valid_time_intervals\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mDatasetCercetare/Videos/Daemahni_on_DaemahniGianna.mov\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m target_audio = AudioSegment.silent(duration=\u001B[32m0\u001B[39m)\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m start_sec, end_sec \u001B[38;5;129;01min\u001B[39;00m valid_intervals:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mget_valid_time_intervals\u001B[39m\u001B[34m(video_path, threshold, min_duration)\u001B[39m\n\u001B[32m     14\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     16\u001B[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m mean_intensity = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgray\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mean_intensity > threshold:  \u001B[38;5;66;03m# Not black\u001B[39;00m\n\u001B[32m     20\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m start_time \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Facultate/An_3/Sem_1/Cercetare/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860\u001B[39m, in \u001B[36mmean\u001B[39m\u001B[34m(a, axis, dtype, out, keepdims, where)\u001B[39m\n\u001B[32m   3857\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3858\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m3860\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_methods\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_mean\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3861\u001B[39m \u001B[43m                      \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Facultate/An_3/Sem_1/Cercetare/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:135\u001B[39m, in \u001B[36m_mean\u001B[39m\u001B[34m(a, axis, dtype, out, keepdims, where)\u001B[39m\n\u001B[32m    132\u001B[39m         dtype = mu.dtype(\u001B[33m'\u001B[39m\u001B[33mf4\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    133\u001B[39m         is_float16_result = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m135\u001B[39m ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, mu.ndarray):\n\u001B[32m    137\u001B[39m     ret = um.true_divide(\n\u001B[32m    138\u001B[39m             ret, rcount, out=ret, casting=\u001B[33m'\u001B[39m\u001B[33munsafe\u001B[39m\u001B[33m'\u001B[39m, subok=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0959729b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:44:33.733563Z",
     "start_time": "2025-11-15T14:43:41.430323Z"
    }
   },
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"small\" # Or \"tiny\", \"base\", \"medium\", \"large-v1\", \"large-v2\", \"large-v3\"\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\") # Adjust device/compute_type as needed\n",
    "\n",
    "segments, info = model.transcribe(\"target_person_audio.wav\", beam_size=5)\n",
    "\n",
    "print(\"Detected language:\", info.language)\n",
    "full_text = \"\"\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "    full_text += segment.text + \" \"\n",
    "\n",
    "print(\"\\nFull Transcription:\", full_text.strip())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/Documents/Facultate/An_3/Sem_1/Cercetare/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "[0.00s -> 0.90s]  I'm nervous as hell.\n",
      "[0.90s -> 1.40s]  Really?\n",
      "[1.40s -> 3.04s]  No, I'm not.\n",
      "[3.04s -> 4.32s]  So you work at a gym.\n",
      "[4.32s -> 5.72s]  Do you like working out?\n",
      "[5.72s -> 6.22s]  I do.\n",
      "[6.22s -> 7.12s]  I love working out.\n",
      "[7.12s -> 8.60s]  I want to become a Pilates instructor.\n",
      "[8.60s -> 9.60s]  That's like my goal.\n",
      "[9.60s -> 10.10s]  Yeah.\n",
      "[10.10s -> 10.60s]  Yeah.\n",
      "[10.60s -> 11.10s]  OK.\n",
      "[11.10s -> 12.08s]  Do you like working out?\n",
      "[12.08s -> 12.48s]  Yes.\n",
      "[12.48s -> 15.84s]  I was actually a personal trainer for a short period of time.\n",
      "[15.84s -> 17.36s]  I kind of just did it as a second job.\n",
      "[17.36s -> 18.04s]  OK.\n",
      "[18.04s -> 20.84s]  Do you have any facts about yourself\n",
      "[20.84s -> 23.84s]  that might be interesting?\n",
      "[23.84s -> 25.08s]  Mm, yeah.\n",
      "[25.08s -> 26.36s]  I haven't been doing it recently,\n",
      "[26.36s -> 27.56s]  but I want to get back into it.\n",
      "[27.56s -> 28.68s]  OK.\n",
      "[28.68s -> 29.64s]  I literally have a camera.\n",
      "[29.68s -> 30.60s]  I have a camera that I bought.\n",
      "[30.60s -> 32.72s]  It was like $600 camera, and I used it like twice.\n",
      "[32.72s -> 33.48s]  For what?\n",
      "[33.48s -> 34.24s]  Photography.\n",
      "[34.24s -> 36.08s]  I tried to make like one video, and it didn't go well.\n",
      "[36.08s -> 38.04s]  And I was like, well, this was waste.\n",
      "[38.04s -> 39.44s]  Oh, videography, not for two people.\n",
      "[39.44s -> 41.56s]  Well, the camera I got, it does both.\n",
      "[41.56s -> 42.80s]  For what would you mean?\n",
      "[42.80s -> 43.56s]  A fruit?\n",
      "[43.56s -> 45.04s]  A fruit.\n",
      "[45.04s -> 45.44s]  Yes.\n",
      "[45.44s -> 47.00s]  Is it bad if I say watermelon?\n",
      "[47.00s -> 50.56s]  Why would that be no?\n",
      "[50.56s -> 51.88s]  No.\n",
      "[51.88s -> 53.36s]  I'll just go with my probably mango.\n",
      "[53.36s -> 54.16s]  I love mangoes.\n",
      "[54.16s -> 54.56s]  Yeah.\n",
      "[54.56s -> 55.88s]  I feel like you would be a mango.\n",
      "[55.88s -> 56.36s]  Right?\n",
      "[56.36s -> 56.72s]  Yeah.\n",
      "[56.72s -> 57.28s]  What would you be?\n",
      "[57.28s -> 59.12s]  You can find the same mangoes are my favorite fruit.\n",
      "[59.20s -> 61.52s]  What do you think I would be?\n",
      "[61.52s -> 62.52s]  Cherries.\n",
      "[62.52s -> 63.04s]  Good answer.\n",
      "[63.04s -> 64.04s]  Good answer.\n",
      "\n",
      "Full Transcription: I'm nervous as hell.  Really?  No, I'm not.  So you work at a gym.  Do you like working out?  I do.  I love working out.  I want to become a Pilates instructor.  That's like my goal.  Yeah.  Yeah.  OK.  Do you like working out?  Yes.  I was actually a personal trainer for a short period of time.  I kind of just did it as a second job.  OK.  Do you have any facts about yourself  that might be interesting?  Mm, yeah.  I haven't been doing it recently,  but I want to get back into it.  OK.  I literally have a camera.  I have a camera that I bought.  It was like $600 camera, and I used it like twice.  For what?  Photography.  I tried to make like one video, and it didn't go well.  And I was like, well, this was waste.  Oh, videography, not for two people.  Well, the camera I got, it does both.  For what would you mean?  A fruit?  A fruit.  Yes.  Is it bad if I say watermelon?  Why would that be no?  No.  I'll just go with my probably mango.  I love mangoes.  Yeah.  I feel like you would be a mango.  Right?  Yeah.  What would you be?  You can find the same mangoes are my favorite fruit.  What do you think I would be?  Cherries.  Good answer.  Good answer.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class AttractionModelWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, window_size=20):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.window = window_size\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        rnn_out, _ = self.rnn(x)  # (B, T, H)\n",
    "\n",
    "        # Local attention: only attend to last `window` steps\n",
    "        T = rnn_out.size(1)\n",
    "        context = rnn_out[:, max(0, T-self.window):, :]  # (B, W', H)\n",
    "        attn_out, _ = self.attn(context, context, context)  # (B, W', H)\n",
    "\n",
    "        # Use last attended state\n",
    "        final_rep = attn_out[:, -1, :]  # (B, H)\n",
    "        attraction = self.head(final_rep)  # (B, 1)\n",
    "        return attraction"
   ],
   "id": "a5ac4ed9a9445bbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
