{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-17T15:28:22.892116Z",
     "start_time": "2025-11-17T15:28:22.887541Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:18:17.392185Z",
     "start_time": "2025-11-17T15:18:17.244342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FinalMultimodalDataProcessor:\n",
    "    def __init__(self, base_path=\"DatasetCercetare\"):\n",
    "        self.base_path = base_path\n",
    "        self.sessions_data = {}\n",
    "\n",
    "    def get_all_sessions(self):\n",
    "        \"\"\"Get all unique session names from the file structure\"\"\"\n",
    "        # Get session names from AudioFeatures (since they're shared)\n",
    "        audio_files = glob.glob(f\"{self.base_path}/AudioFeatures/*.csv\")\n",
    "        sessions = [Path(file).stem for file in audio_files]\n",
    "        return sessions\n",
    "\n",
    "    def get_people_in_session(self, session_name):\n",
    "        \"\"\"Get the people involved in a session from ActionUnits files\"\"\"\n",
    "        au_files = glob.glob(f\"{self.base_path}/ActionUnits/*_on_{session_name}.csv\")\n",
    "        people = []\n",
    "        for file in au_files:\n",
    "            filename = Path(file).stem\n",
    "            person = filename.split('_on_')[0]\n",
    "            people.append(person)\n",
    "        return people\n",
    "\n",
    "    def load_person_data(self, person, session_name):\n",
    "        \"\"\"Load data for a specific person in a session\"\"\"\n",
    "        person_data = {}\n",
    "\n",
    "        print(f\"  Loading data for {person} in session {session_name}\")\n",
    "\n",
    "        # Load Action Units for this person\n",
    "        au_file = f\"{self.base_path}/ActionUnits/{person}_on_{session_name}.csv\"\n",
    "        if Path(au_file).exists():\n",
    "            au_df = pd.read_csv(au_file)\n",
    "            au_features = ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09',\n",
    "                          'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23',\n",
    "                          'AU25', 'AU26', 'AU28']\n",
    "            person_data['action_units'] = au_df[['timestamp'] + au_features]\n",
    "            print(f\"    Action Units: {au_df.shape}\")\n",
    "\n",
    "        # Load Hand Gestures for this person\n",
    "        hg_file = f\"{self.base_path}/HandGestures/{person}_on_{session_name}.csv\"\n",
    "        if Path(hg_file).exists():\n",
    "            hg_df = pd.read_csv(hg_file)\n",
    "            hg_features = ['left_hand_velocity', 'right_hand_velocity',\n",
    "                          'gesture_frequency_cumulative', 'face_touches_cumulative']\n",
    "            person_data['hand_gestures'] = hg_df[['timestamp'] + hg_features]\n",
    "            print(f\"    Hand Gestures: {hg_df.shape}\")\n",
    "\n",
    "        # Load shared Audio Features (same for all people in session)\n",
    "        audio_file = f\"{self.base_path}/AudioFeatures/{session_name}.csv\"\n",
    "        if Path(audio_file).exists():\n",
    "            audio_df = pd.read_csv(audio_file)\n",
    "            audio_features = ['energy_db', 'pitch_hz', 'speaking_rate']\n",
    "            audio_df = audio_df.rename(columns={'time_seconds': 'timestamp'})\n",
    "            person_data['audio'] = audio_df[['timestamp'] + audio_features]\n",
    "            print(f\"    Audio Features: {audio_df.shape}\")\n",
    "\n",
    "        # Load shared Sentiment Analysis (filter by speaker if available)\n",
    "        sent_file = f\"{self.base_path}/SentimentAnalysis/{session_name}.csv\"\n",
    "        if Path(sent_file).exists():\n",
    "            sent_df = pd.read_csv(sent_file)\n",
    "            sent_df = sent_df.rename(columns={'second': 'timestamp'})\n",
    "\n",
    "            # Filter by speaker if the person name matches\n",
    "            if 'speaker' in sent_df.columns:\n",
    "                # Try to match person name with speaker (case insensitive)\n",
    "                person_sent = sent_df[sent_df['speaker'].str.lower() == person.lower()]\n",
    "                if len(person_sent) > 0:\n",
    "                    person_data['sentiment'] = person_sent[['timestamp', 'compound', 'pos', 'neu', 'neg']]\n",
    "                    print(f\"    Sentiment (filtered for {person}): {person_sent.shape}\")\n",
    "                else:\n",
    "                    # If no match, use aggregated sentiment for all speakers\n",
    "                    sent_agg = sent_df.groupby('timestamp').agg({\n",
    "                        'compound': 'mean', 'pos': 'mean', 'neu': 'mean', 'neg': 'mean'\n",
    "                    }).reset_index()\n",
    "                    person_data['sentiment'] = sent_agg\n",
    "                    print(f\"    Sentiment (aggregated): {sent_agg.shape}\")\n",
    "            else:\n",
    "                person_data['sentiment'] = sent_df[['timestamp', 'compound', 'pos', 'neu', 'neg']]\n",
    "                print(f\"    Sentiment: {sent_df.shape}\")\n",
    "\n",
    "        return person_data\n",
    "\n",
    "    def align_person_data(self, person_data, target_fps=1.0):\n",
    "        \"\"\"Align all modalities for a person to the same temporal grid\"\"\"\n",
    "\n",
    "        # Find common time range\n",
    "        min_time = 0\n",
    "        max_time = float('inf')\n",
    "\n",
    "        for modality, data in person_data.items():\n",
    "            if len(data) > 0:\n",
    "                min_time = max(min_time, data['timestamp'].min())\n",
    "                max_time = min(max_time, data['timestamp'].max())\n",
    "\n",
    "        # Create target timeline\n",
    "        target_timeline = np.arange(int(min_time), int(max_time) + 1)\n",
    "        aligned_data = pd.DataFrame({'timestamp': target_timeline})\n",
    "\n",
    "        # Align each modality\n",
    "        for modality, data in person_data.items():\n",
    "            if modality == 'audio':\n",
    "                # Aggregate high-frequency audio to 1-second intervals\n",
    "                audio_agg = data.groupby(data['timestamp'].round()).agg({\n",
    "                    'energy_db': 'mean',\n",
    "                    'pitch_hz': 'mean',\n",
    "                    'speaking_rate': 'mean'\n",
    "                }).reset_index()\n",
    "                aligned_data = aligned_data.merge(audio_agg, on='timestamp', how='left')\n",
    "\n",
    "            else:\n",
    "                # For other modalities, use nearest second matching\n",
    "                data_rounded = data.copy()\n",
    "                data_rounded['timestamp'] = data_rounded['timestamp'].round().astype(int)\n",
    "                data_agg = data_rounded.groupby('timestamp').first().reset_index()\n",
    "                aligned_data = aligned_data.merge(data_agg, on='timestamp', how='left')\n",
    "\n",
    "        # Fill missing values\n",
    "        aligned_data = aligned_data.fillna(method='ffill').fillna(0)\n",
    "\n",
    "        return aligned_data\n",
    "\n",
    "    def process_all_data(self):\n",
    "        \"\"\"Process all sessions and people\"\"\"\n",
    "        sessions = self.get_all_sessions()\n",
    "        print(f\"Found sessions: {sessions}\")\n",
    "\n",
    "        all_processed_data = {}\n",
    "\n",
    "        for session in sessions:\n",
    "            print(f\"\\n=== Processing Session: {session} ===\")\n",
    "            people = self.get_people_in_session(session)\n",
    "            print(f\"People in session: {people}\")\n",
    "\n",
    "            session_data = {}\n",
    "\n",
    "            for person in people:\n",
    "                # Load person's data\n",
    "                person_data = self.load_person_data(person, session)\n",
    "\n",
    "                # Align temporal data\n",
    "                aligned_data = self.align_person_data(person_data)\n",
    "\n",
    "                # Add person and session info\n",
    "                aligned_data['person'] = person\n",
    "                aligned_data['session'] = session\n",
    "\n",
    "                print(f\"    {person} final shape: {aligned_data.shape}\")\n",
    "                print(f\"    {person} features: {[col for col in aligned_data.columns if col not in ['timestamp', 'person', 'session']]}\")\n",
    "\n",
    "                session_data[person] = aligned_data\n",
    "                all_processed_data[f\"{session}_{person}\"] = aligned_data\n",
    "\n",
    "            self.sessions_data[session] = session_data\n",
    "\n",
    "        return all_processed_data\n",
    "\n",
    "# Process all data\n",
    "processor = FinalMultimodalDataProcessor()\n",
    "all_data = processor.process_all_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, data in all_data.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Duration: {data['timestamp'].max() - data['timestamp'].min():.0f} seconds\")\n",
    "    print(f\"  Features: {len([col for col in data.columns if col not in ['timestamp', 'person', 'session']])}\")\n",
    "    print()"
   ],
   "id": "6cccff6487fd23bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sessions: ['DaemahniGianna', 'StephenKeala']\n",
      "\n",
      "=== Processing Session: DaemahniGianna ===\n",
      "People in session: ['Daemahni', 'Gianna']\n",
      "  Loading data for Daemahni in session DaemahniGianna\n",
      "    Action Units: (85, 18)\n",
      "    Hand Gestures: (253, 15)\n",
      "    Audio Features: (3656, 12)\n",
      "    Sentiment (aggregated): (83, 5)\n",
      "    Daemahni final shape: (83, 31)\n",
      "    Daemahni features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "  Loading data for Gianna in session DaemahniGianna\n",
      "    Action Units: (85, 18)\n",
      "    Hand Gestures: (253, 15)\n",
      "    Audio Features: (3656, 12)\n",
      "    Sentiment (aggregated): (83, 5)\n",
      "    Gianna final shape: (83, 31)\n",
      "    Gianna features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "\n",
      "=== Processing Session: StephenKeala ===\n",
      "People in session: ['Keala', 'Stephen']\n",
      "  Loading data for Keala in session StephenKeala\n",
      "    Action Units: (80, 18)\n",
      "    Hand Gestures: (239, 15)\n",
      "    Audio Features: (4527, 12)\n",
      "    Sentiment (aggregated): (87, 5)\n",
      "    Keala final shape: (80, 31)\n",
      "    Keala features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "  Loading data for Stephen in session StephenKeala\n",
      "    Action Units: (101, 18)\n",
      "    Hand Gestures: (301, 15)\n",
      "    Audio Features: (4527, 12)\n",
      "    Sentiment (aggregated): (87, 5)\n",
      "    Stephen final shape: (88, 31)\n",
      "    Stephen features: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']\n",
      "\n",
      "============================================================\n",
      "FINAL DATA SUMMARY\n",
      "============================================================\n",
      "DaemahniGianna_Daemahni:\n",
      "  Shape: (83, 31)\n",
      "  Duration: 82 seconds\n",
      "  Features: 28\n",
      "\n",
      "DaemahniGianna_Gianna:\n",
      "  Shape: (83, 31)\n",
      "  Duration: 82 seconds\n",
      "  Features: 28\n",
      "\n",
      "StephenKeala_Keala:\n",
      "  Shape: (80, 31)\n",
      "  Duration: 79 seconds\n",
      "  Features: 28\n",
      "\n",
      "StephenKeala_Stephen:\n",
      "  Shape: (88, 31)\n",
      "  Duration: 87 seconds\n",
      "  Features: 28\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:22:06.111049Z",
     "start_time": "2025-11-17T15:22:06.105342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ground_truth = {\n",
    "    'session_person': ['DaemahniGianna_Daemahni', 'DaemahniGianna_Gianna',\n",
    "                       'StephenKeala_Keala', 'StephenKeala_Stephen'],\n",
    "    'is_attracted': [1, 1, 0, 0]  # Based on your ground truth\n",
    "}\n",
    "\n",
    "ground_truth_df = pd.DataFrame(ground_truth)\n",
    "print(\"Ground Truth Labels:\")\n",
    "print(ground_truth_df)\n",
    "print()"
   ],
   "id": "8e765c39d32a47d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Labels:\n",
      "            session_person  is_attracted\n",
      "0  DaemahniGianna_Daemahni             1\n",
      "1    DaemahniGianna_Gianna             1\n",
      "2       StephenKeala_Keala             0\n",
      "3     StephenKeala_Stephen             0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:24:28.799202Z",
     "start_time": "2025-11-17T15:24:28.750806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultimodalDataPreprocessor:\n",
    "    \"\"\"Handles data preprocessing and normalization for multimodal attraction data\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length=15):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def create_sequences(self, all_data, ground_truth_df):\n",
    "        \"\"\"Create sequences for RNN training\"\"\"\n",
    "        X_sequences = []\n",
    "        y_labels = []\n",
    "        sequence_info = []\n",
    "\n",
    "        print(\"Creating sequences...\")\n",
    "\n",
    "        for key, data in all_data.items():\n",
    "            # Get label for this person\n",
    "            label_row = ground_truth_df[ground_truth_df['session_person'] == key]\n",
    "            if len(label_row) == 0:\n",
    "                continue\n",
    "\n",
    "            label = label_row['is_attracted'].iloc[0]\n",
    "\n",
    "            # Remove non-feature columns\n",
    "            feature_data = data.drop(['timestamp', 'person', 'session'], axis=1)\n",
    "\n",
    "            # Store feature names (from first dataset)\n",
    "            if self.feature_names is None:\n",
    "                self.feature_names = feature_data.columns.tolist()\n",
    "\n",
    "            # Create overlapping sequences\n",
    "            for i in range(len(feature_data) - self.sequence_length + 1):\n",
    "                sequence = feature_data.iloc[i:i + self.sequence_length].values\n",
    "                X_sequences.append(sequence)\n",
    "                y_labels.append(label)\n",
    "                sequence_info.append({\n",
    "                    'person': key,\n",
    "                    'start_time': i,\n",
    "                    'end_time': i + self.sequence_length - 1\n",
    "                })\n",
    "\n",
    "        X = np.array(X_sequences)\n",
    "        y = np.array(y_labels)\n",
    "\n",
    "        print(f\"Created {len(X)} sequences\")\n",
    "        print(f\"Sequence shape: {X.shape}\")\n",
    "        print(f\"Features: {len(self.feature_names)}\")\n",
    "        print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "        return X, y, sequence_info\n",
    "\n",
    "    def fit_normalizer(self, X_train):\n",
    "        \"\"\"Fit the feature normalizer on training data\"\"\"\n",
    "        print(\"Fitting feature normalizer...\")\n",
    "\n",
    "        # Reshape for normalization (samples*time, features)\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "        # Fit scaler\n",
    "        self.feature_scaler.fit(X_train_reshaped)\n",
    "        self.is_fitted = True\n",
    "\n",
    "        print(\"Feature normalizer fitted!\")\n",
    "        return self\n",
    "\n",
    "    def normalize_features(self, X):\n",
    "        \"\"\"Normalize features using fitted scaler\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Normalizer not fitted! Call fit_normalizer() first.\")\n",
    "\n",
    "        # Reshape for normalization\n",
    "        original_shape = X.shape\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "\n",
    "        # Transform\n",
    "        X_normalized = self.feature_scaler.transform(X_reshaped)\n",
    "        X_normalized = X_normalized.reshape(original_shape)\n",
    "\n",
    "        return X_normalized\n",
    "\n",
    "    def split_by_person(self, X, y, sequence_info, validation_split=0.2):\n",
    "        \"\"\"Split data by person to avoid data leakage\"\"\"\n",
    "        print(\"Splitting data by person...\")\n",
    "\n",
    "        # Group sequences by person\n",
    "        person_sequences = {}\n",
    "        for i, info in enumerate(sequence_info):\n",
    "            person = info['person']\n",
    "            if person not in person_sequences:\n",
    "                person_sequences[person] = []\n",
    "            person_sequences[person].append(i)\n",
    "\n",
    "        # Split by person\n",
    "        train_indices = []\n",
    "        val_indices = []\n",
    "\n",
    "        for person, indices in person_sequences.items():\n",
    "            n_val = max(1, int(len(indices) * validation_split))\n",
    "            val_indices.extend(indices[-n_val:])  # Last sequences for validation\n",
    "            train_indices.extend(indices[:-n_val])  # Rest for training\n",
    "\n",
    "        X_train = X[train_indices]\n",
    "        X_val = X[val_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_val = y[val_indices]\n",
    "\n",
    "        print(f\"Train set: {len(X_train)} sequences\")\n",
    "        print(f\"Val set: {len(X_val)} sequences\")\n",
    "        print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
    "        print(f\"Val class distribution: {np.bincount(y_val)}\")\n",
    "\n",
    "        return X_train, X_val, y_train, y_val, train_indices, val_indices\n",
    "\n",
    "    def prepare_training_data(self, all_data, ground_truth_df, validation_split=0.2):\n",
    "        \"\"\"Complete data preparation pipeline\"\"\"\n",
    "        # Create sequences\n",
    "        X, y, sequence_info = self.create_sequences(all_data, ground_truth_df)\n",
    "\n",
    "        # Split by person\n",
    "        X_train, X_val, y_train, y_val, train_idx, val_idx = self.split_by_person(\n",
    "            X, y, sequence_info, validation_split\n",
    "        )\n",
    "\n",
    "        # Fit normalizer on training data\n",
    "        self.fit_normalizer(X_train)\n",
    "\n",
    "        # Normalize both sets\n",
    "        X_train_norm = self.normalize_features(X_train)\n",
    "        X_val_norm = self.normalize_features(X_val)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train_norm,\n",
    "            'X_val': X_val_norm,\n",
    "            'y_train': y_train,\n",
    "            'y_val': y_val,\n",
    "            'train_indices': train_idx,\n",
    "            'val_indices': val_idx,\n",
    "            'sequence_info': sequence_info\n",
    "        }\n",
    "\n",
    "    def preprocess_new_data(self, person_data):\n",
    "        \"\"\"Preprocess new data for prediction\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor not fitted! Train model first.\")\n",
    "\n",
    "        # Remove non-feature columns\n",
    "        feature_data = person_data.drop(['timestamp', 'person', 'session'], axis=1, errors='ignore')\n",
    "\n",
    "        # Create sequences\n",
    "        sequences = []\n",
    "        for i in range(len(feature_data) - self.sequence_length + 1):\n",
    "            sequence = feature_data.iloc[i:i + self.sequence_length].values\n",
    "            sequences.append(sequence)\n",
    "\n",
    "        if len(sequences) == 0:\n",
    "            raise ValueError(f\"Not enough data points. Need at least {self.sequence_length} time steps.\")\n",
    "\n",
    "        X = np.array(sequences)\n",
    "        X_normalized = self.normalize_features(X)\n",
    "\n",
    "        return X_normalized\n"
   ],
   "id": "8fd5872cf8d9fc21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PREPROCESSING\n",
      "============================================================\n",
      "Creating sequences...\n",
      "Created 278 sequences\n",
      "Sequence shape: (278, 15, 28)\n",
      "Features: 28\n",
      "Class distribution: [140 138]\n",
      "Splitting data by person...\n",
      "Train set: 225 sequences\n",
      "Val set: 53 sequences\n",
      "Train class distribution: [113 112]\n",
      "Val class distribution: [27 26]\n",
      "Fitting feature normalizer...\n",
      "Feature normalizer fitted!\n",
      "\n",
      "Preprocessing complete!\n",
      "Training data shape: (225, 15, 28)\n",
      "Validation data shape: (53, 15, 28)\n",
      "Feature names: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06']... (showing first 5)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:30:50.695058Z",
     "start_time": "2025-11-17T15:30:50.663580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize preprocessor\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "preprocessor = MultimodalDataPreprocessor(sequence_length=15)\n",
    "data_dict = preprocessor.prepare_training_data(all_data, ground_truth_df)\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n",
    "print(f\"Training data shape: {data_dict['X_train'].shape}\")\n",
    "print(f\"Validation data shape: {data_dict['X_val'].shape}\")\n",
    "print(f\"Feature names: {preprocessor.feature_names[:28]}... (showing first 5)\")"
   ],
   "id": "32e8e73d5d69dba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PREPROCESSING\n",
      "============================================================\n",
      "Creating sequences...\n",
      "Created 278 sequences\n",
      "Sequence shape: (278, 15, 28)\n",
      "Features: 28\n",
      "Class distribution: [140 138]\n",
      "Splitting data by person...\n",
      "Train set: 225 sequences\n",
      "Val set: 53 sequences\n",
      "Train class distribution: [113 112]\n",
      "Val class distribution: [27 26]\n",
      "Fitting feature normalizer...\n",
      "Feature normalizer fitted!\n",
      "\n",
      "Preprocessing complete!\n",
      "Training data shape: (225, 15, 28)\n",
      "Validation data shape: (53, 15, 28)\n",
      "Feature names: ['AU01', 'AU02', 'AU04', 'AU05', 'AU06', 'AU07', 'AU09', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU20', 'AU23', 'AU25', 'AU26', 'AU28', 'left_hand_velocity', 'right_hand_velocity', 'gesture_frequency_cumulative', 'face_touches_cumulative', 'energy_db', 'pitch_hz', 'speaking_rate', 'compound', 'pos', 'neu', 'neg']... (showing first 5)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:29:48.018951Z",
     "start_time": "2025-11-17T15:29:48.007329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttractionLSTMModel:\n",
    "    \"\"\"LSTM model for attraction prediction\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build LSTM model architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            # First LSTM layer\n",
    "            LSTM(64, return_sequences=True, input_shape=input_shape, dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            # Second LSTM layer\n",
    "            LSTM(32, return_sequences=True, dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            # Third LSTM layer\n",
    "            LSTM(16, dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "\n",
    "            # Dense layers\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=16):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if self.model is None:\n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "            self.build_model(input_shape)\n",
    "\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        self.model.summary()\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=20, restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Handle class imbalance\n",
    "        class_weight = None\n",
    "        if len(np.unique(y_train)) > 1:\n",
    "            from sklearn.utils.class_weight import compute_class_weight\n",
    "            classes = np.unique(y_train)\n",
    "            weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "            class_weight = {classes[i]: weights[i] for i in range(len(classes))}\n",
    "            print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "        print(\"\\nTraining model...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained!\")\n",
    "        return self.model.evaluate(X, y)\n"
   ],
   "id": "57f8f5fb4c4a93",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:33:09.763299Z",
     "start_time": "2025-11-17T15:32:51.628288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "attraction_model = AttractionLSTMModel()\n",
    "history = attraction_model.train(\n",
    "    data_dict['X_train'],\n",
    "    data_dict['y_train'],\n",
    "    data_dict['X_val'],\n",
    "    data_dict['y_val']\n",
    ")"
   ],
   "id": "154865fd3b583800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │        \u001B[38;5;34m23,808\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m32\u001B[0m)         │        \u001B[38;5;34m12,416\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m32\u001B[0m)         │           \u001B[38;5;34m128\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)             │         \u001B[38;5;34m3,136\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)             │            \u001B[38;5;34m64\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │           \u001B[38;5;34m544\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)             │           \u001B[38;5;34m528\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m17\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m40,897\u001B[0m (159.75 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,897</span> (159.75 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m40,673\u001B[0m (158.88 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,673</span> (158.88 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m224\u001B[0m (896.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.995575221238938, 1: 1.0044642857142858}\n",
      "\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 63ms/step - accuracy: 0.5156 - loss: 0.7281 - precision: 0.5238 - recall: 0.2946 - val_accuracy: 0.8113 - val_loss: 0.6677 - val_precision: 0.7222 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.7244 - loss: 0.5705 - precision: 0.7604 - recall: 0.6518 - val_accuracy: 0.9434 - val_loss: 0.6382 - val_precision: 0.8966 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.8356 - loss: 0.4629 - precision: 0.9121 - recall: 0.7411 - val_accuracy: 0.9623 - val_loss: 0.5956 - val_precision: 0.9286 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.8844 - loss: 0.3841 - precision: 0.9574 - recall: 0.8036 - val_accuracy: 0.9623 - val_loss: 0.5280 - val_precision: 0.9286 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9156 - loss: 0.2880 - precision: 0.9115 - recall: 0.9196 - val_accuracy: 0.9811 - val_loss: 0.4127 - val_precision: 0.9630 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.9556 - loss: 0.2360 - precision: 0.9904 - recall: 0.9196 - val_accuracy: 1.0000 - val_loss: 0.3017 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9600 - loss: 0.1928 - precision: 0.9640 - recall: 0.9554 - val_accuracy: 0.8679 - val_loss: 0.3236 - val_precision: 0.7879 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9556 - loss: 0.1516 - precision: 0.9474 - recall: 0.9643 - val_accuracy: 0.8679 - val_loss: 0.2987 - val_precision: 0.7879 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.9733 - loss: 0.1265 - precision: 0.9818 - recall: 0.9643 - val_accuracy: 0.9057 - val_loss: 0.2192 - val_precision: 0.8387 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9778 - loss: 0.1009 - precision: 0.9820 - recall: 0.9732 - val_accuracy: 0.9434 - val_loss: 0.1386 - val_precision: 0.8966 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0855 - precision: 0.9910 - recall: 0.9821 - val_accuracy: 0.9245 - val_loss: 0.1201 - val_precision: 0.8667 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.1116 - precision: 0.9910 - recall: 0.9821 - val_accuracy: 0.9434 - val_loss: 0.1098 - val_precision: 0.8966 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9733 - loss: 0.0782 - precision: 0.9818 - recall: 0.9643 - val_accuracy: 0.9245 - val_loss: 0.1300 - val_precision: 0.8667 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9778 - loss: 0.1082 - precision: 0.9820 - recall: 0.9732 - val_accuracy: 0.9434 - val_loss: 0.0886 - val_precision: 0.8966 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9778 - loss: 0.0959 - precision: 0.9735 - recall: 0.9821 - val_accuracy: 0.9245 - val_loss: 0.0953 - val_precision: 0.8667 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9733 - loss: 0.1184 - precision: 0.9732 - recall: 0.9732 - val_accuracy: 0.9245 - val_loss: 0.1016 - val_precision: 0.8667 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9733 - loss: 0.0831 - precision: 0.9818 - recall: 0.9643 - val_accuracy: 0.9434 - val_loss: 0.0653 - val_precision: 0.8966 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9733 - loss: 0.0847 - precision: 0.9649 - recall: 0.9821 - val_accuracy: 0.9434 - val_loss: 0.0713 - val_precision: 0.8966 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9867 - loss: 0.0478 - precision: 0.9823 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.1953 - val_precision: 0.8125 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9822 - loss: 0.0697 - precision: 0.9737 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.1687 - val_precision: 0.8125 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9956 - loss: 0.0388 - precision: 1.0000 - recall: 0.9911 - val_accuracy: 0.9245 - val_loss: 0.1135 - val_precision: 0.8667 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9956 - loss: 0.0641 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.9057 - val_loss: 0.1370 - val_precision: 0.8387 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 1.0000 - loss: 0.0206 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9057 - val_loss: 0.1198 - val_precision: 0.8387 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0580 - precision: 0.9825 - recall: 1.0000 - val_accuracy: 0.9057 - val_loss: 0.1361 - val_precision: 0.8387 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9778 - loss: 0.1061 - precision: 0.9652 - recall: 0.9911 - val_accuracy: 0.9057 - val_loss: 0.1972 - val_precision: 1.0000 - val_recall: 0.8077 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0399 - precision: 0.9911 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.2982 - val_precision: 1.0000 - val_recall: 0.7692 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0723 - precision: 0.9910 - recall: 0.9821 - val_accuracy: 0.8868 - val_loss: 0.3019 - val_precision: 1.0000 - val_recall: 0.7692 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.9956 - loss: 0.0173 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.8868 - val_loss: 0.2393 - val_precision: 1.0000 - val_recall: 0.7692 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9822 - loss: 0.1062 - precision: 1.0000 - recall: 0.9643 - val_accuracy: 0.8868 - val_loss: 0.2442 - val_precision: 1.0000 - val_recall: 0.7692 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9822 - loss: 0.0704 - precision: 0.9737 - recall: 0.9911 - val_accuracy: 0.9057 - val_loss: 0.2141 - val_precision: 1.0000 - val_recall: 0.8077 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0508 - precision: 1.0000 - recall: 0.9732 - val_accuracy: 0.9623 - val_loss: 0.0746 - val_precision: 1.0000 - val_recall: 0.9231 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0306 - precision: 0.9911 - recall: 0.9911 - val_accuracy: 1.0000 - val_loss: 0.0495 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0383 - precision: 0.9825 - recall: 1.0000 - val_accuracy: 0.9811 - val_loss: 0.0615 - val_precision: 0.9630 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9956 - loss: 0.0225 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.9623 - val_loss: 0.0852 - val_precision: 0.9286 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0316 - precision: 0.9823 - recall: 0.9911 - val_accuracy: 0.9623 - val_loss: 0.0998 - val_precision: 0.9286 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0610 - precision: 0.9739 - recall: 1.0000 - val_accuracy: 0.9623 - val_loss: 0.1187 - val_precision: 1.0000 - val_recall: 0.9231 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9911 - loss: 0.0323 - precision: 0.9911 - recall: 0.9911 - val_accuracy: 0.9245 - val_loss: 0.1824 - val_precision: 1.0000 - val_recall: 0.8462 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9911 - loss: 0.0340 - precision: 0.9911 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.2277 - val_precision: 0.9167 - val_recall: 0.8462 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0482 - precision: 0.9910 - recall: 0.9821 - val_accuracy: 0.8868 - val_loss: 0.2365 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 1.0000 - loss: 0.0193 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9057 - val_loss: 0.2022 - val_precision: 0.8889 - val_recall: 0.9231 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9956 - loss: 0.0206 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.9057 - val_loss: 0.2094 - val_precision: 0.8889 - val_recall: 0.9231 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9867 - loss: 0.0255 - precision: 0.9823 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.2515 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9867 - loss: 0.0345 - precision: 0.9910 - recall: 0.9821 - val_accuracy: 0.8868 - val_loss: 0.2601 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0301 - precision: 0.9825 - recall: 1.0000 - val_accuracy: 0.8868 - val_loss: 0.2419 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9956 - loss: 0.0190 - precision: 1.0000 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.2230 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9956 - loss: 0.0161 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.8868 - val_loss: 0.2296 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9956 - loss: 0.0201 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.8868 - val_loss: 0.2461 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 48/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0614 - precision: 1.0000 - recall: 0.9821 - val_accuracy: 0.8868 - val_loss: 0.2429 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 1.0000 - loss: 0.0143 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.8868 - val_loss: 0.2211 - val_precision: 0.8846 - val_recall: 0.8846 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9956 - loss: 0.0101 - precision: 0.9912 - recall: 1.0000 - val_accuracy: 0.9057 - val_loss: 0.2187 - val_precision: 0.8889 - val_recall: 0.9231 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9911 - loss: 0.0818 - precision: 0.9911 - recall: 0.9911 - val_accuracy: 0.9057 - val_loss: 0.2108 - val_precision: 0.8889 - val_recall: 0.9231 - learning_rate: 2.5000e-04\n",
      "Epoch 52/100\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9956 - loss: 0.0213 - precision: 1.0000 - recall: 0.9911 - val_accuracy: 0.8868 - val_loss: 0.2988 - val_precision: 0.8571 - val_recall: 0.9231 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T15:35:14.480988Z",
     "start_time": "2025-11-17T15:35:13.412886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss = min(attraction_model.history.history['val_loss'])\n",
    "best_val_acc = max(attraction_model.history.history['val_accuracy'])\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nFinal model performance:\")\n",
    "val_loss, val_acc, val_precision, val_recall = attraction_model.evaluate(\n",
    "    data_dict['X_val'],\n",
    "    data_dict['y_val'],\n",
    ")\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Test predictions on validation set\n",
    "val_predictions = attraction_model.predict(data_dict['X_val'])\n",
    "val_pred_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(f\"Predicted 0 (not attracted): {np.sum(val_pred_binary == 0)}\")\n",
    "print(f\"Predicted 1 (attracted): {np.sum(val_pred_binary == 1)}\")\n",
    "print(f\"Actual 0 (not attracted): {np.sum(data_dict['y_val'] == 0)}\")\n",
    "print(f\"Actual 1 (attracted): {np.sum(data_dict['y_val'] == 1)}\")"
   ],
   "id": "4d317cd3b90db395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.0495\n",
      "Best validation accuracy: 1.0000\n",
      "\n",
      "Final model performance:\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 1.0000 - loss: 0.0495 - precision: 1.0000 - recall: 1.0000\n",
      "Validation Loss: 0.0495\n",
      "Validation Accuracy: 1.0000\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 1.0000\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 546ms/step\n",
      "\n",
      "Prediction distribution:\n",
      "Predicted 0 (not attracted): 27\n",
      "Predicted 1 (attracted): 26\n",
      "Actual 0 (not attracted): 27\n",
      "Actual 1 (attracted): 26\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "63a98251e7b07a50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
