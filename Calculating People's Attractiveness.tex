\documentclass{article}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\begin{document}

\title{\Large \textbf{Calculating people’s attraction based on their facial expressions, body movement, voice tone and words spoken}\\
\normalsize{Mitrea Ioan David} 
}

\maketitle
\section{Data}

\subsection{Data Source}
The dataset used in this research consists of publicly available YouTube videos showing 
first-time interactions between two people, primarily in speed-dating contexts. 
These videos typically contain short conversations (3--5 minutes) where participants decide 
whether they would like to go on a second date. This final decision serves as the 
ground-truth label (\textit{positive attraction} = 1, \textit{no attraction} = 0).

This approach is supported by previous research showing that real-time recordings of 
natural interactions can reliably capture social and emotional cues 
\citep{haidet2009}. Furthermore, speed-dating scenarios have been used to study romantic 
interest and body-movement synchrony \citep{chang2021}, as well as trait-rated 
attraction in virtual dating environments \citep{french2024}.

\subsection{Modalities Collected}

Following multimodal affective-computing research 
\citep{kraack2023, sherstinsky2022, 2017multimodalmachinelearningsurvey},
four complementary modalities are extracted:

\begin{itemize}
    \item \textbf{Facial expressions}: frame-level emotional and micro-expression features.
    \item \textbf{Body movements}: upper-body posture, gesture frequency, and motion energy.
    \item \textbf{Voice tone}: prosodic signals---pitch, intensity, vibrato, and spectral shape.
    \item \textbf{Spoken language}: utterance segmentation and semantic embeddings extracted 
    from speech-to-text transcripts.
\end{itemize}

These modalities align with the theoretical view that attraction is multimodal, 
influenced simultaneously by appearance, vocal qualities, movement, and language 
\citep{williams2023, schirmer2025, didonato2023}.

\subsection{Data Preprocessing}

\subsubsection{Video Preprocessing}
Videos are sampled at fixed intervals (1\,s, 2\,s, and 5\,s) to test different temporal granularities.
Black frames—introduced by video editing—are removed from the \emph{visual stream} but \textbf{audio is preserved}.  
This ensures that vocal features remain accessible even when visual frames are invalid.

Feature extraction follows affective computing best practices 
\citep{sherstinsky2022}, and motion cues are computed analogously to 
speed-dating movement studies \citep{chang2021}.

\subsubsection{Audio Preprocessing}
Audio is diarized into two speakers using pretrained separation models, following 
multimodal speech guidelines \citep{ahlawat2022}.  
This yields:

\begin{itemize}
    \item Speaker A prosody features
    \item Speaker B prosody features
    \item Speaker A transcript embeddings
    \item Speaker B transcript embeddings
\end{itemize}

This separation is crucial because the model computes attraction \emph{per individual}, not jointly.

\subsubsection{Text Processing}
Speech is transcribed using a transformer-based ASR system, as recommended in 
recent ASR surveys \citep{ahlawat2022}.  
Sentences are encoded using modern semantic embeddings to capture warmth, 
interest, and affective vocabulary.

\subsection{Labels}

The label for each interaction is extracted from the explicit verbal response of the 
participants (“yes/no” to a second date).  
These labels serve as ground truth for binary attraction classification.

Since no prior multimodal dataset explicitly measures attraction, our contribution 
constitutes the first dataset of its kind, bridging multimodal emotion recognition 
\citep{kraack2023} and romantic-attraction analysis \citep{didonato2023}.

\section{Methodology}

\subsection{Overview}
The proposed approach is a multimodal sequential model that integrates facial expressions, 
body movement, vocal tone, and linguistic features to predict attraction during first-time 
interactions. The model follows the principle that emotions and social signals evolve 
temporally and are influenced by previously expressed states 
\citep{lebois2018, schmidt2019}.

\subsection{Model Architecture}

\subsubsection{Parallel Feature Extractors}
Each modality is processed through a dedicated encoder operating at the same sampling rate:

\begin{itemize}
    \item \textbf{Facial Expression Encoder}: a CNN-based feature extractor, following 
    multimodal emotion-recognition architectures \citep{kraack2023}.
    
    \item \textbf{Body Movement Encoder}: a motion-vector and pose-estimation module using 
    standard affective-computing visual pipelines \citep{sherstinsky2022}.
    
    \item \textbf{Voice Tone Encoder}: an audio CNN or Transformer extracting prosodic features, 
    following speech emotion research \citep{ahlawat2022}.
    
    \item \textbf{Language Encoder}: Transformer-based embeddings of the transcript, 
    similar to multimodal sentiment-analysis frameworks \citep{zadeh2017tensorfusionnetworkmultimodal}.
\end{itemize}

\subsubsection{Fusion and Alignment}
All modality embeddings at time $t$ are concatenated into a unified multimodal vector:
\[
x_t = [x_t^{\text{face}}, x_t^{\text{body}}, x_t^{\text{voice}}, x_t^{\text{text}}].
\]

Temporal alignment is performed using:
\begin{itemize}
    \item strict timestamp matching (baseline),
    \item sliding-window alignment as proposed in SWRR \citep{zhao2023swr},
    \item tensor fusion for cross-modal interaction \citep{zadeh2017tensorfusionnetworkmultimodal}.
\end{itemize}

This allows us to evaluate the sensitivity of the model to synchronization choices, 
as recommended in multimodal fusion surveys \citep{2017multimodalmachinelearningsurvey}.

\subsection{Sequential Modeling}
A recurrent neural network (RNN, LSTM, or GRU) processes the multimodal sequence:
\[
h_t = f(h_{t-1}, x_t),
\]
where $h_t$ represents the latent \emph{interaction state} at time $t$, reflecting gradual 
emotional buildup, consistent with theories of situated emotion \citep{lebois2018}.

Following \cite{schmidt2019}, an LSTM architecture is preferred due to its capacity to model 
long-term dependencies without vanishing gradients.

\subsection{Output Layer}
After the final frame $T$, the model produces a global attraction score:
\[
\hat{y} = \sigma(W h_T + b),
\]
classifying the interaction as ``attracted’’ (1) or ``not attracted’’ (0).

\subsection{Validation and Evaluation}

\subsubsection{Comparison Baselines}
Because no prior work explicitly predicts attraction from all four modalities, we compare our model with:

\begin{itemize}
    \item unimodal baselines (face-only, body-only, voice-only, text-only),
    \item Tensor Fusion Network \citep{zadeh2017tensorfusionnetworkmultimodal},
    \item sliding-window attention models \citep{zhao2023swr}.
\end{itemize}

This tests whether multimodal fusion provides measurable performance gains, as suggested by 
multimodal-attraction studies \citep{williams2023, schirmer2025}.

\subsubsection{Evaluation Metrics}

\begin{itemize}
    \item Accuracy
    \item Precision, recall, F1
    \item ROC-AUC
    \item Temporal consistency (smoothness of predictions)
    \item Cross-modal correlation scores
\end{itemize}

\subsubsection{Experimental Variants}

To validate robustness, we vary:
\begin{itemize}
    \item \textbf{frame rate}: 1\,s, 2\,s, 5\,s
    \item \textbf{sequence length}: full interaction vs. early-segment prediction
    \item \textbf{fusion strategies}: early vs. late fusion vs. tensor fusion
    \item \textbf{alignment methods}: strict vs. sliding window
\end{itemize}

\subsection{Mathematical Model}

For each modality $m \in \{\text{}{face}, \text{body}, \text{voice}, \text{text}\}$ and time step $t$:

\[
x_t^m = E_m(I_t^m)
\]

All modality embeddings are fused as:

\[
x_t = x_t^{\text{face}} +  x_t^{\text{body}} + x_t^{\text{voice}} +  x_t^{\text{text}}
\]

The RNN updates its hidden state via:

\[
h_t = \mathrm{LSTM}(x_t, h_{t-1})
\]

Final attraction probability:

\[
P(y=1 \mid x_{1:T}) = \sigma(Wh_T + b)
\]

Loss function (binary cross-entropy):

\[
\mathcal{L} = - \left[y \log(\hat{y}) + (1-y)\log(1-\hat{y})\right]
\]

This provides a complete mathematical formulation of the proposed experimental model.


\bibliographystyle{unsrt}
\bibliography{refs}

\end{document}