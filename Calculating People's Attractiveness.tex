\documentclass{article}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amssymb}

\begin{document}

\title{\Large \textbf{Calculating people’s attraction based on their facial expressions, body movement, voice tone and words spoken}\\
\normalsize{Mitrea Ioan David} 
}

\maketitle

\section{Data Collection} 
The quality and reliability of affective computing research depend critically on rigorous and multimodal data collection pipelines capable of capturing complex human emotional expressions. Traditional laboratory-based acquisition frameworks prioritize controlled environments and high-fidelity measurements; however, they often suffer from limited ecological validity and reduced generalizability \citep{schmidt2019}. Recent research has therefore shifted toward hybrid approaches that integrate audiovisual, physiological, and behavioral modalities collected in more naturalistic settings, thereby enabling richer affective modeling \citep{lebois2018}. Following this methodological direction, our study implements a multimodal extraction pipeline designed to capture fine-grained facial, acoustic, and bodily behavioral cues from video recordings. The complete pipeline is summarized below. 
\subsection{Person-Specific Video Segmentation} To ensure that downstream feature extraction is performed only during time intervals in which the target participant is visible, we implemented person-specific temporal segmentation based on facial recognition. For each video frame, we computed face embeddings using \texttt{face\_recognition} and compared them to a reference embedding of the target participant. A cosine distance threshold determined whether the detected face corresponded to the participant of interest. This yielded a set of visibility intervals \[ S = \{ (t_i^{\text{start}}, t_i^{\text{end}}) \mid i = 1,\dots,N \}, \] representing all temporal segments in which the participant was visually present. These intervals functioned as the structural backbone for subsequent audio and behavioral feature extraction. 
\subsection{Audio Extraction Aligned With Speaker Visibility} Because conventional speaker diarization models can be unreliable in multi-speaker naturalistic environments, we leveraged the person-visibility segments \(S\) to construct a participant-specific audio track. Using \texttt{moviepy}, audio was extracted only from the intervals in which the participant was detected onscreen. Silent padding ensured the temporal alignment of the resulting WAV file with the original video duration. This visibility-constrained audio track facilitates the analysis of vocal affect—such as prosody, energy, and temporal speaking patterns—while minimizing cross-speaker contamination. 
\subsection{Facial Action Unit Extraction} Facial behavior was captured by extracting facial Action Units (AUs) based on a two-stage mapping process. First, emotion probabilities were obtained using the DeepFace facial expression recognizer. Second, these categorical emotion estimates were mapped to continuous AU vectors using a rule-based mapping derived from established facial expression literature. Each frame within the visibility intervals produced a 17-dimensional AU vector: \[ \mathbf{a}_t = [\mathrm{AU1}_t, \ldots, \mathrm{AU17}_t]. \] Frames containing no detectable face (e.g., motion blur, occlusion) were assigned the zero vector to preserve temporal coherence. This procedure enabled robust modeling of dynamic facial affect across the entire temporal sequence. 
\subsection{Body, Hand, and Upper-Body Movement Features} To capture nonverbal expressive behaviors beyond the face, we employed MediaPipe's FaceMesh, Hands, and Pose modules to extract fine-grained kinematic and gestural information. For each frame, we tracked 3D keypoints for the hands, face contour, and upper body. Time-aligned features included: 
\begin{itemize} 
    \item \textbf{Hand movement dynamics}: frame-to-frame velocity, acceleration, and movement smoothness. 
    \item \textbf{Gesture frequency}: counts of meaningful movements exceeding a motion threshold. 
    \item \textbf{Face-touch gestures}: detection of hand-to-face contact events using spatial proximity between hand landmarks and facial meshes. 
    \item \textbf{Upper-body motion}: displacement and temporal derivatives of shoulder, torso, and arm keypoints. 
\end{itemize} 
By integrating these multimodal cues—facial expression, vocal behavior, and gesture/movement dynamics—we obtain a rich representation of affective behavior capable of supporting downstream analyses such as affect recognition, behavioral profiling, and interpersonal dynamics modeling. This multimodal pipeline aligns with contemporary trends in affective computing that emphasize ecological validity, multimodal integration, and fine-grained temporal annotation \citep{schmidt2019, lebois2018}.

\section{Methodology}

Our approach models short temporal sequences of multimodal behavioral signals to predict binary romantic-interest labels. The methodology consists of per-second feature aggregation, sequence construction, preprocessing, and sequential LSTM-based modeling. This section provides a precise description of each step, including hyperparameters and architectural details.

\subsection{Sequence Construction and Feature Representation}

Per-participant video recordings are preprocessed to produce \textbf{per-second feature vectors} $\mathbf{x}_t \in \mathbb{R}^{28}$ comprising:

\begin{itemize}
    \item 17 \textbf{Facial Action Units (AUs)}: frame-level categorical emotions are mapped via a rule-based scheme to continuous AU intensities, then averaged within each second. Missing frames (no face detected) are zero-filled before averaging.
    \item 4 \textbf{Hand and upper-body movement features}: left/right hand velocity, cumulative gesture frequency, and cumulative face-touch events computed from MediaPipe landmarks.
    \item 3 \textbf{Acoustic features}: energy (dB), pitch (Hz), and speaking rate extracted from participant-specific audio segments constrained by visual presence.
    \item 4 \textbf{Sentiment features}: compound, positive, neutral, and negative sentiment scores aggregated from manual-adjusted ScribeFlow transcripts for utterances overlapping each second. Zero-filled when participant is silent.
\end{itemize}

These per-second vectors are concatenated to form sequences of length $T=15$ seconds:

\[
\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T] \in \mathbb{R}^{T \times 28}.
\]

Sequences are generated with a sliding window (stride 1) to capture temporal dependencies, producing overlapping samples for model training. Missing modalities are handled deterministically via zero-filling, allowing the model to learn from absent or occluded signals without introducing NaNs or imputed values.

\subsection{Feature Normalization}

All features are standardized using a \textbf{StandardScaler (z-score normalization)} fit on the training set. For efficiency, the training sequences are reshaped from $(N_\text{train}, T, D)$ to $(N_\text{train}\cdot T, D)$ prior to fitting. Validation and test sequences are transformed using the same scaler. This ensures consistent normalization without leaking information from held-out data.

\subsection{Sequential LSTM Architecture}

The model is a stacked LSTM followed by fully connected layers:

\begin{itemize}
    \item \textbf{Input:} sequences of shape $(T=15, D=28)$.  
    \item \textbf{LSTM layers:}
    \begin{enumerate}
        \item LSTM(64), \texttt{return\_sequences=True}, dropout=0.2  
        \item BatchNormalization
        \item LSTM(32), \texttt{return\_sequences=True}, dropout=0.2  
        \item BatchNormalization
        \item LSTM(16), \texttt{return\_sequences=False}, dropout=0.2  
        \item BatchNormalization
    \end{enumerate}
    \item \textbf{Dense layers:} Dense(32) with ReLU + Dropout(0.3), Dense(16) with ReLU + Dropout(0.2)  
    \item \textbf{Output:} Dense(1) with sigmoid activation, producing probability $\hat{y}\in[0,1]$
\end{itemize}

Formally, the model learns a mapping:

\[
f_\theta: \mathbb{R}^{T\times D} \to [0,1], \qquad \hat{y} = f_\theta(\mathbf{X}),
\]

where $\theta$ denotes all learnable parameters.

\subsection{Training Objective and Optimization}

The model is trained to minimize \textbf{binary cross-entropy}:

\[
\mathcal{L}(\theta) = - \frac{1}{N} \sum_{i=1}^{N} w_{y_i} \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right],
\]

where $w_{y_i}$ are optional class weights computed from the training data to address class imbalance. The optimizer is \textbf{Adam}, with default parameters (learning rate = 0.001).

\subsection{Training Protocol}

Training uses:

\begin{itemize}
    \item \textbf{Batch size:} 16  
    \item \textbf{Epochs:} up to 100 with EarlyStopping (monitor=\texttt{val\_loss}, patience=20, restore\_best\_weights=True)  
    \item \textbf{Learning rate reduction:} ReduceLROnPlateau (factor=0.5, patience=10, min\_lr=1e-6)  
    \item \textbf{Metrics:} accuracy, precision, recall
\end{itemize}

Person-specific sequences are split such that validation data contains the last 20\% of each participant’s sequences, preventing data leakage and ensuring evaluation reflects generalization to unseen individuals.


\subsection{Summary}

This methodology integrates multimodal behavioral signals into a sequential modeling framework. Temporal aggregation, sequence construction, normalization, and LSTM-based learning collectively enable robust prediction of romantic interest while preserving interpretability and reproducibility. The approach adheres to best practices in affective computing and sequence-based behavioral modeling \citep{lebois2018, schmidt2019}.

\section{References}

\bibliographystyle{unsrt}
\bibliography{refs}

\end{document}