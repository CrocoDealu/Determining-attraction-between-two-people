{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import face_recognition\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class AudioSegmentExtractor:\n",
    "    def __init__(self):\n",
    "        self.face_encodings = []\n",
    "        self.person_segments = []\n",
    "\n",
    "    def load_reference_image(self, image_path: str) -> bool:\n",
    "        \"\"\"Load reference image of the person to detect\"\"\"\n",
    "        try:\n",
    "            # Load reference image\n",
    "            reference_image = face_recognition.load_image_file(image_path)\n",
    "            face_encodings = face_recognition.face_encodings(reference_image)\n",
    "\n",
    "            if len(face_encodings) == 0:\n",
    "                print(\"No face found in reference image!\")\n",
    "                return False\n",
    "\n",
    "            self.face_encodings = face_encodings\n",
    "            print(f\"Loaded reference face from {image_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading reference image: {e}\")\n",
    "            return False\n",
    "\n",
    "    def detect_person_in_video(self, video_path: str, tolerance: float = 0.6) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Detect segments where the target person appears in video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        print(f\"Processing video: {frame_count} frames at {fps} FPS\")\n",
    "\n",
    "        person_present = []\n",
    "        frame_number = 0\n",
    "\n",
    "        # Process every nth frame to speed up detection\n",
    "        skip_frames = max(1, int(fps // 2))  # Process 2 frames per second\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_number % skip_frames == 0:\n",
    "                # Convert BGR to RGB\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Find faces in current frame\n",
    "                face_locations = face_recognition.face_locations(rgb_frame)\n",
    "                face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "                # Check if target person is in frame\n",
    "                person_found = False\n",
    "                for face_encoding in face_encodings:\n",
    "                    matches = face_recognition.compare_faces(self.face_encodings, face_encoding, tolerance=tolerance)\n",
    "                    if True in matches:\n",
    "                        person_found = True\n",
    "                        break\n",
    "\n",
    "                timestamp = frame_number / fps\n",
    "                person_present.append((timestamp, person_found))\n",
    "\n",
    "                if frame_number % (skip_frames * 30) == 0:  # Progress update every ~15 seconds\n",
    "                    progress = (frame_number / frame_count) * 100\n",
    "                    print(f\"Progress: {progress:.1f}%\")\n",
    "\n",
    "            frame_number += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Convert detection points to segments\n",
    "        segments = self._create_segments(person_present, fps)\n",
    "        self.person_segments = segments\n",
    "\n",
    "        print(f\"Found {len(segments)} segments where person is present\")\n",
    "        return segments\n",
    "\n",
    "    def _create_segments(self, detections: List[Tuple[float, bool]], fps: float) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Convert detection points to continuous segments\"\"\"\n",
    "        segments = []\n",
    "        current_start = None\n",
    "\n",
    "        for timestamp, person_present in detections:\n",
    "            if person_present and current_start is None:\n",
    "                current_start = timestamp\n",
    "            elif not person_present and current_start is not None:\n",
    "                segments.append((current_start, timestamp))\n",
    "                current_start = None\n",
    "\n",
    "        # Handle case where person is present until end\n",
    "        if current_start is not None:\n",
    "            segments.append((current_start, detections[-1][0]))\n",
    "\n",
    "        # Merge nearby segments (within 1 second)\n",
    "        merged_segments = []\n",
    "        for start, end in segments:\n",
    "            if merged_segments and start - merged_segments[-1][1] < 1.0:\n",
    "                merged_segments[-1] = (merged_segments[-1][0], end)\n",
    "            else:\n",
    "                merged_segments.append((start, end))\n",
    "\n",
    "        return merged_segments\n",
    "\n",
    "    def extract_audio_segments(self, source_audio_path: str, output_path: str,\n",
    "                             total_duration: float = None) -> str:\n",
    "        \"\"\"Extract audio segments and create final audio file\"\"\"\n",
    "        try:\n",
    "            # Load source audio\n",
    "            print(f\"Loading source audio: {source_audio_path}\")\n",
    "            source_audio = AudioSegment.from_file(source_audio_path)\n",
    "\n",
    "            # Get total duration from video if not provided\n",
    "            if total_duration is None:\n",
    "                # Try to get duration from the first video file\n",
    "                # This is a fallback - ideally pass duration from video analysis\n",
    "                total_duration = len(source_audio) / 1000.0\n",
    "\n",
    "            # Create silent audio of total duration\n",
    "            silent_audio = AudioSegment.silent(duration=int(total_duration * 1000))\n",
    "\n",
    "            print(f\"Creating audio file of {total_duration:.2f} seconds\")\n",
    "            print(f\"Extracting {len(self.person_segments)} segments\")\n",
    "\n",
    "            # Extract and overlay segments\n",
    "            for i, (start_time, end_time) in enumerate(self.person_segments):\n",
    "                print(f\"Processing segment {i+1}: {start_time:.2f}s - {end_time:.2f}s\")\n",
    "\n",
    "                # Convert to milliseconds\n",
    "                start_ms = int(start_time * 1000)\n",
    "                end_ms = int(end_time * 1000)\n",
    "                duration_ms = end_ms - start_ms\n",
    "\n",
    "                # Extract segment from source audio\n",
    "                if start_ms < len(source_audio):\n",
    "                    if end_ms <= len(source_audio):\n",
    "                        audio_segment = source_audio[start_ms:end_ms]\n",
    "                    else:\n",
    "                        # Handle case where segment extends beyond source audio\n",
    "                        audio_segment = source_audio[start_ms:]\n",
    "                        # Pad with silence if needed\n",
    "                        needed_silence = duration_ms - len(audio_segment)\n",
    "                        if needed_silence > 0:\n",
    "                            audio_segment += AudioSegment.silent(duration=needed_silence)\n",
    "                else:\n",
    "                    # If start is beyond source audio, use silence\n",
    "                    audio_segment = AudioSegment.silent(duration=duration_ms)\n",
    "\n",
    "                # Overlay onto silent audio\n",
    "                silent_audio = silent_audio.overlay(audio_segment, position=start_ms)\n",
    "\n",
    "            # Export final audio\n",
    "            print(f\"Exporting final audio to: {output_path}\")\n",
    "            silent_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "            return output_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting audio segments: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_video_duration(self, video_path: str) -> float:\n",
    "        \"\"\"Get video duration in seconds\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = frame_count / fps\n",
    "        cap.release()\n",
    "        return duration\n",
    "\n",
    "def select_file(title: str, filetypes: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Open file dialog to select a file\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=title,\n",
    "        filetypes=filetypes\n",
    "    )\n",
    "\n",
    "    root.destroy()\n",
    "    return file_path\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Extract audio segments based on person detection in video\")\n",
    "    parser.add_argument(\"--video\", help=\"Path to input video file\")\n",
    "    parser.add_argument(\"--reference\", help=\"Path to reference image of person\")\n",
    "    parser.add_argument(\"--audio\", help=\"Path to source audio file\")\n",
    "    parser.add_argument(\"--output\", help=\"Path to output audio file\", default=\"extracted_audio.wav\")\n",
    "    parser.add_argument(\"--tolerance\", type=float, default=0.6, help=\"Face recognition tolerance (0.0-1.0)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize extractor\n",
    "    extractor = AudioSegmentExtractor()\n",
    "\n",
    "    # Get video file\n",
    "    video_path = args.video\n",
    "    if not video_path:\n",
    "        print(\"Select video file:\")\n",
    "        video_path = select_file(\"Select Video File\", [\n",
    "            (\"Video files\", \"*.mp4 *.avi *.mov *.mkv *.wmv\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ])\n",
    "\n",
    "    if not video_path or not os.path.exists(video_path):\n",
    "        print(\"Invalid video file selected!\")\n",
    "        return\n",
    "\n",
    "    # Get reference image\n",
    "    reference_path = args.reference\n",
    "    if not reference_path:\n",
    "        print(\"Select reference image of the person:\")\n",
    "        reference_path = select_file(\"Select Reference Image\", [\n",
    "            (\"Image files\", \"*.jpg *.jpeg *.png *.bmp\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ])\n",
    "\n",
    "    if not reference_path or not os.path.exists(reference_path):\n",
    "        print(\"Invalid reference image selected!\")\n",
    "        return\n",
    "\n",
    "    # Load reference image\n",
    "    if not extractor.load_reference_image(reference_path):\n",
    "        return\n",
    "\n",
    "    # Detect person in video\n",
    "    print(\"Analyzing video for person detection...\")\n",
    "    segments = extractor.detect_person_in_video(video_path, tolerance=args.tolerance)\n",
    "\n",
    "    if not segments:\n",
    "        print(\"No segments found where the person is present!\")\n",
    "        return\n",
    "\n",
    "    # Print detected segments\n",
    "    print(\"\\nDetected segments:\")\n",
    "    total_segment_time = 0\n",
    "    for i, (start, end) in enumerate(segments):\n",
    "        duration = end - start\n",
    "        total_segment_time += duration\n",
    "        print(f\"  Segment {i+1}: {start:.2f}s - {end:.2f}s (duration: {duration:.2f}s)\")\n",
    "\n",
    "    print(f\"Total time with person present: {total_segment_time:.2f}s\")\n",
    "\n",
    "    # Get source audio file\n",
    "    audio_path = args.audio\n",
    "    if not audio_path:\n",
    "        print(\"\\nSelect source audio file:\")\n",
    "        audio_path = select_file(\"Select Source Audio File\", [\n",
    "            (\"Audio files\", \"*.wav *.mp3 *.m4a *.flac *.aac\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ])\n",
    "\n",
    "    if not audio_path or not os.path.exists(audio_path):\n",
    "        print(\"Invalid audio file selected!\")\n",
    "        return\n",
    "\n",
    "    # Get video duration\n",
    "    video_duration = extractor.get_video_duration(video_path)\n",
    "    print(f\"Video duration: {video_duration:.2f}s\")\n",
    "\n",
    "    # Extract audio segments\n",
    "    print(\"\\nExtracting audio segments...\")\n",
    "    output_path = extractor.extract_audio_segments(audio_path, args.output, video_duration)\n",
    "\n",
    "    if output_path:\n",
    "        print(f\"\\nSuccess! Extracted audio saved to: {output_path}\")\n",
    "        print(f\"The output audio will have sound only during the {len(segments)} segments where the person is visible.\")\n",
    "    else:\n",
    "        print(\"Failed to extract audio segments!\")"
   ],
   "id": "a4f38547ca93ac10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
